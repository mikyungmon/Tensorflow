# Chapter 12

**DQN**은 딥마인드에서 나온 신경망이다. DQN은 게임 화면만 보고 게임을 학습하는 신경망으로 2014년에 공개했다.

딥마인드에서 이 DQN으로 아타리 2600용 비디오 게임 49개를 학습시킨 결과 모두 잘 학습하여 플레이했고 그중 29개에서는 사람의 평균 기록보다 높은 점수를 보였다.

이번 장에서는 DQN의 개념을 간단히 살펴보고 직접 구현한 뒤 간단한 게임을 학습시켜본다.

## 12.1 DQN 개념

**DQN은 Deep Q-network의 줄임말인데 강화학습 알고리즘으로 유명한 Q-러닝을 딥러닝으로 구현했다는 의미이다.**

💡 강화학습이란? 

  - 어떤 환경에서 인공지능 에이전트가 현재 상태(환경)를 판단하여 가장 이로운 행동을 만드는 학습 방법이다.
  
  - 학습 시 이로운 행동을 하면 보상을 해주고 해로운 행동을 하면 페널티를 줘서 학습이 진행될 수 있도록 이로운 행동을 점점 많이 하도록 유도한다.
  
  - 즉 누적된 이득이 최대가 되게 행동하도록 학습이 진행된다.

**Q-러닝**은 어떠한 상태에서 특정 행동을 했을 때의 가치를 나타내는 함수인 **Q 함수**를 학습하는 알고리즘이다.

즉, 특정 상태에서 이 함수의 값이 최대가 되는 행동을 찾도록 학습하면 그 상태에서 어떤 행동을 취해야 할지 알 수 있게 된다.

그리고 이 Q함수를 신경망을 활용해 학습하게 한 것이 바로 DQN이다.

하지만 Q-러닝을 신경망으로 구현하면 학습이 상당히 불안정해진다.

이에 딥마인드는 다음의 두 가지 방법을 사용하여 이 문제를 해결하였다.

1) 먼저 과거의 상태를 기억한 뒤 그중에서 임의의 상태를 뽑아 학습시키는 방법을 사용한다. 이렇게 하면 특수한 상황에 치우치지 않도록 학습시킬 수 있어서 더 좋은 결과를 내는데 도움이 된다.

2) 두 번째로는 손실값 계산을 위해 학습을 진행하면서 최적의 행동을 얻어내는 **기본 신경망**과 얻어낸 값이 좋은 선택인지 비교하는 **목표(target) 신경망**을 분리하는 방법을 사용한다. 목표 신경망은 계속 갱신하는 것이 아니라 기본 신경망의 학습된 결과값을 일정 주기마다 목표 신경망에 갱신해준다.

그리고 DQN은 화면의 상태, 즉 화면 영상만으로 게임을 학습한다. 따라서 이미지 인식에 뛰어난 CNN을 사용하여 신경망 모델을 구성하였다.

(사진) DQN기본개념

## 12.2 게임 소개

OpenAI는 인공지능의 발전을 위해 다양한 실험을 할 수 있는 Gym이라는 강화학습 알고리즘 개발 도구를 제공한다.

이 도구를 이용하면 아타리 게임들을 쉽게 구동할 수 있다. 다만, 아타리 게임을 학습시키려면 매우 성능 좋은 컴퓨터가 필요하고 시간도 아주 오래걸린다.

시간이 너무 오래 걸리는 환경은 공부용으로 적절하지 않아서 학습을 빠르게 시켜볼 수 있는 다음 그림과 같은 간단한 게임을 사용한다.

(사진)

이 게임은 아래로 떨어지는 물체를 피하는 간단한 게임이다. 

소스는 깃허브 저장소에서 내려받을 수 있다(https://goo.gl/VQ9JDT).

인터페이스는 일부러 OpenAI Gym과 거의 같게 만들었다.

## 12.3 에이전트 구현하기

**에이전트**는 게임의 상태를 입력받아 신경망으로 전달하고 신경망에서 판단한 행동을 게임에 적용해서 다음 단계로 진행한다.

그러므로 에이전트가 어떤 식으로 작동하는지 알아야 신경망 구현 시 이해가 더 수월할 것이기 때문에 신경망 모델을 구현하기 전에 에이전트부터 구현해본다.

1️⃣
