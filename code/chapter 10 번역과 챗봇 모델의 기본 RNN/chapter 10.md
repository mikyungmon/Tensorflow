# Chapter 10

ì´ë¯¸ì§€ ì¸ì‹ì— CNNì´ ìˆë‹¤ë©´ ìì—°ì–´ ì¸ì‹ì—ëŠ” **ìˆœí™˜ ì‹ ê²½ë§**ì´ë¼ê³  í•˜ëŠ” RNN(Recurrent Neural Network)ê°€ ìˆë‹¤.

RNNì€ ìƒíƒœê°€ ê³ ì •ëœ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë‹¤ë¥¸ ì‹ ê²½ë§ê³¼ëŠ” ë‹¬ë¦¬ ìì—°ì–´ ì²˜ë¦¬ë‚˜ ìŒì„± ì¸ì‹ì²˜ëŸ¼ **ìˆœì„œê°€ ìˆëŠ” ë°ì´í„°**ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë° ê°•ì ì„ ê°€ì§„ ì‹ ê²½ë§ì´ë‹¤.

ì•ì´ë‚˜ ë’¤ì˜ ì •ë³´ì— ë”°ë¼ ì „ì²´ì˜ ì˜ë¯¸ê°€ ë‹¬ë¼ì§€ê±°ë‚˜ ì•ì˜ ì •ë³´ë¡œ ë‹¤ìŒì— ë‚˜ì˜¬ ì •ë³´ë¥¼ ì¶”ì¸¡í•˜ë ¤ëŠ” ê²½ìš°ì— RNNì„ ì‚¬ìš©í•˜ë©´ ì„±ëŠ¥ ì¢‹ì€ í”„ë¡œê·¸ë¨ì„ ë§Œë“¤ ìˆ˜ ìˆë‹¤.

ì´ë²ˆ ì¥ì—ì„œëŠ” RNNì˜ ê¸°ë³¸ì ì¸ ì‚¬ìš©ë²•ì„ ë°°ìš°ê³  ë§ˆì§€ë§‰ì—ëŠ” Sequence to Sequenceëª¨ë¸ì„ ì´ìš©í•´ ê°„ë‹¨í•œ ë²ˆì—­ í”„ë¡œê·¸ë¨ì„ ë§Œë“ ë‹¤.

## 10.1 MNISTë¥¼ RNNìœ¼ë¡œ

ì•ì—ì„œ ì‚¬ìš©í•´ì˜¨ ì†ê¸€ì”¨ ì´ë¯¸ì§€ë¥¼ RNNë°©ì‹ìœ¼ë¡œ í•™ìŠµí•˜ê³  ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ì„ ë§Œë“¤ì–´ë³´ì.

ê¸°ë³¸ì ì¸ RNN ê°œë…ì€ ë‹¤ìŒ ê·¸ë¦¼ê³¼ ê°™ë‹¤.

![image](https://user-images.githubusercontent.com/66320010/127316303-679ead38-17d1-454a-af2a-31e4bfe0005f.png)

- ì´ ê·¸ë¦¼ì˜ ê°€ìš´ë°ì— ìˆëŠ” í•œ ë©ì–´ë¦¬ì˜ ì‹ ê²½ë§ì„ RNNì—ì„œëŠ” **ì…€**ì´ë¼ê³  í•˜ë©° RNNì€ ì´ ì…€ì„ ì—¬ëŸ¬ê°œ ì¤‘ì²©í•˜ì—¬ ì‹¬ì¸µ ì‹ ê²½ë§ì„ ë§Œë“ ë‹¤.

- ê°„ë‹¨í•˜ê²Œ ë§í•´ ì• ë‹¨ê³„ì—ì„œ í•™ìŠµí•œ ê²°ê³¼ë¥¼ ë‹¤ìŒ ë‹¨ê³„ì˜ í•™ìŠµì—ë„ ì´ìš©í•˜ëŠ” ê²ƒì¸ë°, ì´ëŸ° êµ¬ì¡°ë¡œ ì¸í•´ í•™ìŠµ ë°ì´í„°ë¥¼ ë‹¨ê³„ë³„ë¡œ êµ¬ë¶„í•˜ì—¬ ì…ë ¥í•´ì•¼ í•œë‹¤. ë”°ë¼ì„œ MNISTì˜ ì…ë ¥ê°’ë„ ë‹¨ê³„ë³„ë¡œ ì…ë ¥í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë³€ê²½í•´ì¤˜ì•¼í•œë‹¤.

![image](https://user-images.githubusercontent.com/66320010/127316346-157abb34-67aa-4bb4-8bea-db5d8cb38748.png)

- ì‚¬ëŒì€ ê¸€ì”¨ë¥¼ ìœ„ì—ì„œ ì•„ë˜ë¡œ ë‚´ë ¤ê°€ë©´ì„œ ì“°ëŠ” ê²½í–¥ì´ ë§ìœ¼ë‹ˆ ë°ì´í„°ë¥¼ ìœ„ ê·¸ë¦¼ì²˜ëŸ¼ êµ¬ì„±í•œë‹¤.

- MNISTê°€ ê°€ë¡œ,ì„¸ë¡œ 28 * 28í¬ê¸°ì´ë‹ˆ ê°€ë¡œ í•œ ì¤„ì˜ 28í”½ì…€ì„ í•œ ë‹¨ê³„ì˜ ì…ë ¥ê°’ìœ¼ë¡œ ì‚¼ê³  ì„¸ë¡œì¤„ì´ ì´ 28ê°œì´ë¯€ë¡œ 28ë‹¨ê³„ë¥¼ ê±°ì³ ë°ì´í„°ë¥¼ ì…ë ¥ë°›ëŠ” ê°œë…ì´ë‹¤.

1ï¸âƒ£ ì½”ë“œë¥¼ ì‘ì„±í•´ë³¸ë‹¤. ë‹¤ìŒì€ í•™ìŠµì— ì‚¬ìš©í•  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë“¤ê³¼ ë³€ìˆ˜, ì¶œë ¥ì¸µì„ ìœ„í•œ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ ì •ì˜í•œ ë¶€ë¶„ì´ë‹¤.

    import tensorflow as tf
    
    from tensorflow.examples.tutorials.mnist import input_data
    mnist = input_data.read_data_sets("./mnist/data/", one_hot = True)
    
    learning_rate = 0.001
    total_epoch = 30
    batch_size = 128
    
    n_input = 28
    n_step = 28
    n_hidden = 128
    n_class = 10
    
    X = tf.placeholder(tf.float32, [None, n_step, n_input])
    Y = tf.placeholder(tf.float32, [None, n_class])
    
    W = tf.Variable(tf.random_normal([n_hidden,n_class]))
    b = tf.Variable(tf.random_normal([n_class]))
    
 - ê¸°ì¡´ ëª¨ë¸ê³¼ ë‹¤ë¥¸ ì ì€ ì…ë ¥ê°’ Xì— n_stepì´ë¼ëŠ” ì°¨ì›ì„ í•˜ë‚˜ ì¶”ê°€í•œ ë¶€ë¶„ì´ë‹¤. RNNì€ ìˆœì„œê°€ ìˆëŠ” ë°ì´í„°ë¥¼ ë‹¤ë£¨ë¯€ë¡œ í•œ ë²ˆì— ì…ë ¥ ë°›ì„ ê°œìˆ˜ì™€ ì´ ëª‡ ë‹¨ê³„ë¡œ ì´ë¤„ì§„ ë°ì´í„°ë¥¼ ë°›ì„ì§€ ì„¤ì •í•´ì•¼ í•œë‹¤.
 
 - ì¶œë ¥ê°’ì€ MNISTì˜ ë¶„ë¥˜ì¸ 0~9ê¹Œì§€ 10ê°œì˜ ìˆ«ìë¥¼ ì›-í•« ì¸ì½”ë”©ìœ¼ë¡œ í‘œí˜„í•˜ë„ë¡ ë§Œë“¤ì—ˆë‹¤.

2ï¸âƒ£ ê·¸ëŸ° ë‹¤ìŒ n_hiddenê°œì˜ ì¶œë ¥ê°’ì„ ê°–ëŠ” RNNì…€ì„ ìƒì„±í•œë‹¤. RNNì„ ì €ìˆ˜ì¤€ë¶€í„° ì§ì ‘ êµ¬í˜„í•˜ë ¤ë©´ ë‹¤ë¥¸ ì‹ ê²½ë§ë³´ë‹¤ ë³µì¡í•œ ê³„ì‚°ì„ ê±°ì³ì•¼ í•˜ì§€ë§Œ í…ì„œí”Œë¡œë¥¼ ì´ìš©í•˜ë©´ ë‹¤ìŒì²˜ëŸ¼ ë§¤ìš° ê°„ë‹¨í•˜ê²Œ ìƒì„±í•  ìˆ˜ ìˆë‹¤.

    cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)
    
  - RNN ê¸°ë³¸ ì‹ ê²½ë§ì€ ê¸´ ë‹¨ê³„ì˜ ë°ì´í„°ë¥¼ í•™ìŠµí•  ë•Œ ë§¨ ë’¤ì—ì„œëŠ” ë§¨ ì•ì˜ ì •ë³´ë¥¼ ì˜ ê¸°ì–µí•˜ì§€ ëª»í•˜ëŠ” íŠ¹ì„±ì´ ìˆë‹¤. ì´ë¥¼ ë³´ì™„í•˜ê¸° ìœ„í•´ ë‹¤ì–‘í•œ êµ¬ì¡°ê°€ ë§Œë“¤ì–´ì¡Œê³  ê·¸ ì¤‘ ë§ì´ ì‚¬ìš©ë˜ëŠ” ê²ƒì´ **LSTM**ì´ë¼ëŠ” ì‹ ê²½ë§ì´ë‹¤.
  
  - GRUëŠ” LSTMê³¼ ë¹„ìŠ·í•˜ì§€ë§Œ êµ¬ì¡°ê°€ ì¡°ê¸ˆ ë” ê°„ë‹¨í•œ ì‹ ê²½ë§ ì•„í‚¤í…ì²˜ì´ë‹¤.

3ï¸âƒ£ ë‹¤ìŒìœ¼ë¡œ dynamic_rnní•¨ìˆ˜ë¥¼ ì´ìš©í•´ RNN ì‹ ê²½ë§ì„ ì™„ì„±í•œë‹¤.

    outputs, states = tf.nn.dynamic_rnn(cell, X, dtype = tf.float32)
    
 - ì•ì„œ ìƒì„±í•œ RNN ì…€ê³¼ ì…ë ¥ê°’, ê·¸ë¦¬ê³  ì…ë ¥ê°’ì˜ ìë£Œí˜•ì„ ë„£ì–´ì£¼ê¸°ë§Œ í•˜ë©´ ê°„ë‹¨í•˜ê²Œ ì‹ ê²½ë§ ìƒì„±í•  ìˆ˜ ìˆë‹¤.

 - ì•ì˜ RNN ê·¸ë¦¼ì²˜ëŸ¼ í•œ ë‹¨ê³„ë¥¼ í•™ìŠµí•œ ë’¤ ìƒíƒœë¥¼ ì €ì¥í•˜ê³  ê·¸ ìƒíƒœë¥¼ ë‹´ì€ ë‹¨ê³„ì˜ ì…ë ¥ ìƒíƒœë¡œ í•˜ì—¬ ë‹¤ì‹œ í•™ìŠµí•œë‹¤. 
 
 - ì´ë ‡ê²Œ ì£¼ì–´ì§„ ë‹¨ê³„ë§Œí¼ ë°˜ë³µí•˜ì—¬ ìƒíƒœë¥¼ ì „íŒŒí•˜ë©´ì„œ ì¶œë ¥ê°’ì„ ë§Œë“¤ì–´ê°€ëŠ” ê²ƒì´ RNNì˜ ê¸°ë³¸ êµ¬ì¡°ì´ë‹¤.
 
 - í•˜ì§€ë§Œ ë°˜ë³µ ë‹¨ê³„ì—ì„œ ê³ ë ¤í•´ì•¼ í•  ê²ƒì´ ë§ê¸° ë•Œë¬¸ì— ìš°ë¦¬ëŠ” ì´ ê³¼ì •ì„ ëŒ€ì‹  í•´ì£¼ëŠ” dynamic_rnní•¨ìˆ˜ë¥¼ ì‚¬ìš©í–ˆë‹¤. ì´ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë©´ ë‹¨ ë‘ì¤„ë¡œ RNN ëª¨ë¸ì˜ í•µì‹¬ êµ¬ì¡°(ì…€ê³¼ ì‹ ê²½ë§)ë¥¼ ë§Œë“¤ ìˆ˜ ìˆë‹¤.

4ï¸âƒ£ RNNì—ì„œ ë‚˜ì˜¨ ì¶œë ¥ê°’ì„ ê°€ì§€ê³  ìµœì¢… ì¶œë ¥ê°’ì„ ë§Œë“¤ì–´ë³¸ë‹¤.

ê²°ê³¼ê°’ì„ ì›-í•« ì¸ì½”ë”© í˜•íƒœë¡œ ë§Œë“¤ ê²ƒì´ê¸° ë•Œë¬¸ì— ì†ì‹¤ í•¨ìˆ˜ë¡œ tf.nn.softmax_cross_entropy_with_logitsì„ ì‚¬ìš©í•œë‹¤. 

ì´ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ ìµœì¢… ê²°ê³¼ê°’ì´ ì‹¤ì¸¡ê°’ Yì™€ ë™ì¼í•œ í˜•íƒœì¸ [batch_size, n_class]ì—¬ì•¼ í•œë‹¤. 

ì•ì—ì„œ ì´ í˜•íƒœì˜ ì¶œë ¥ê°’ì„ ë§Œë“¤ê¸° ìœ„í•´ì„œ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ ë‹¤ìŒê³¼ ê°™ì´ ì„¤ì •í–ˆë‹¤.

    W = tf.Variable(tf.random_normal([n_hidden,n_class]))
    b = tf.Variable(tf.random_normal([n_class]))
    
ê·¸ëŸ°ë° RNN ì‹ ê²½ë§ì—ì„œ ë‚˜ì˜¤ëŠ” ì¶œë ¥ê°’ì€ ê° ë‹¨ê³„ê°€ í¬í•¨ëœ **[batch_size, n_step, n_hidden] í˜•íƒœ**ì´ë‹¤.

ë”°ë¼ì„œ ì€ë‹‰ì¸µì˜ ì¶œë ¥ê°’ì„ ê°€ì¤‘ì¹˜ Wì™€ ê°™ì€ í˜•íƒœë¡œ ë§Œë“¤ì–´ì¤˜ì•¼ í–‰ë ¬ê³±ì„ ìˆ˜í–‰í•˜ì—¬ ì›í•˜ëŠ” ì¶œë ¥ê°’ì„ ì–»ì„ ìˆ˜ ìˆë‹¤.

(ì°¸ê³ ë¡œ dynamic_rnn í•¨ìˆ˜ì˜ ì˜µì…˜ ì¤‘ time_majorì˜ ê°’ì„ Trueë¡œ í•˜ë©´ [batch_size, n_step, n_hidden] í˜•íƒœë¡œ ì¶œë ¥ëœë‹¤.)

    # outputs : [batch_size, n_step, n_hidden] -> [ n_step, batch_size, n_hidden]
    outputs = tf.transpose(outputs,[1,0,2])   # n_stepê³¼ batch_size ì°¨ì›ì˜ ìˆœì„œë¥¼ ë°”ê¿ˆ
    # outputs : [batch_size, n_hidden]
    outputs = outputs[-1]  # n_step ì°¨ì› ì œê±° -> ë§ˆì§€ë§‰ ë‹¨ê³„ì˜ ê²°ê³¼ê°’ë§Œ ì·¨í•¨
    
5ï¸âƒ£ ì´ì œ ì¸ê³µì‹ ê²½ë§ì˜ ê¸°ë³¸ ìˆ˜ì‹ì´ì í•µì‹¬ì¸ y = X * W + bë¥¼ ì´ìš©í•˜ì—¬ ìµœì¢… ê²°ê³¼ê°’ì„ ë§Œë“ ë‹¤.

    model = tf.matmul(outputs,W) + b
    
6ï¸âƒ£ ì§€ê¸ˆê¹Œì§€ ë§Œë“  ëª¨ë¸ê³¼ ì‹¤ì¸¡ê°’ì„ ë¹„êµí•˜ì—¬ ì†ì‹¤ê°’ì„ êµ¬í•˜ê³  ì‹ ê²½ë§ì„ ìµœì í™”í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹ ê²½ë§ êµ¬ì„±ì„ ë§ˆë¬´ë¦¬í•œë‹¤.

    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = model, label = Y))
    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)

7ï¸âƒ£ ì•ì„œ êµ¬ì„±í•œ ì‹ ê²½ë§ì„ í•™ìŠµì‹œí‚¤ê³  ê²°ê³¼ë¥¼ í™•ì¸í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•œë‹¤. ì• ì¥ì˜ ì½”ë“œì™€ ê±°ì˜ ê°™ì§€ë§Œ ì…ë ¥ê°’ì´ [batch_size, n_step,n_input]í˜•íƒœì´ë¯€ë¡œ CNNì—ì„œ ì‚¬ìš©í•œ ê²ƒì²˜ëŸ¼ reshapeí•¨ìˆ˜ë¥¼ ì´ìš©í•´ ë°ì´í„° í˜•íƒœë¥¼ ë°”ê¿”ì£¼ëŠ” ë¶€ë¶„ë§Œ ì£¼ì˜í•˜ë©´ ëœë‹¤.

    sess = tf.Session()
    sess.run(tf.global_variables_initializer())
    
    total_batch = int(mnist.train.num_exmples / batch_size)
    
    for epoch in range(total_epoch):
        total_cost = 0
        
        for i in range(total_batch):
            batch_xs, batch_ys = mnist.train.next_batch(batch_size)
            batch_xs = batch_xs.reshape((batch_size,n_step, n_input))
        
            _, cost_val = sess.run([optimizer,cost], feed_dict = {X:batch_xs, Y: batch_ys})
        
            total_cost += cost_val
        
        print('Epoch:', '%04d' %(epoch+1), 'Avg. cost = ', '{:.3f}'.format(total_cost / total_batch))
      
    print('ìµœì í™” ì™„ë£Œ!')
    
    is_correct = tf.equal(tf.argmax(model,1), tf.argmax(Y,1))
    accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))
    
    test_batch_size = len(mnist.test.images)
    test_xs = mnist.test.images.reshape(test_batch_size, n_step, n_input)
    test_ys = mnist.test.labels
    
    print('ì •í™•ë„:', sess.run(accuracy, feed_dict = {X:test_xs, Y: test_ys}))
    
## 10.2 ë‹¨ì–´ ìë™ ì™„ì„±

ì´ë²ˆì—ëŠ” RNN ëª¨ë¸ì„ ì´ìš©í•˜ì—¬ ë‹¨ì–´ë¥¼ ìë™ ì™„ì„±í•˜ëŠ” í”„ë¡œê·¸ë¨ì„ ë§Œë“¤ì–´ë³¸ë‹¤.

ì˜ë¬¸ì 4ê°œë¡œ êµ¬ì„±ëœ ë‹¨ì–´ë¥¼ í•™ìŠµì‹œì¼œ 3ê¸€ìë§Œ ì£¼ì–´ì§€ë©´ ë‚˜ë¨¸ì§€ í•œ ê¸€ìë¥¼ ì¶”ì²œí•˜ì—¬ ë‹¨ì–´ë¥¼ ì™„ì„±í•˜ëŠ” í”„ë¡œê·¸ë¨ì´ë‹¤.

ì°¸ê³ ë¡œ dynamic_rnnì˜ sequence_lengthì˜µì…˜ì„ ì‚¬ìš©í•˜ë©´ ê°€ë³€ ê¸¸ì´ ë‹¨ì–´ë¥¼ í•™ìŠµì‹œí‚¬ ìˆ˜ ìˆë‹¤. 

ì§§ì€ ë‹¨ì–´ëŠ” ê°€ì¥ ê¸´ ë‹¨ì–´ì˜ ê¸¸ì´ ë§Œí¼ ë’·ë¶€ë¶„ì„ 0ìœ¼ë¡œ ì±„ìš°ê³  í•´ë‹¹ ë‹¨ì–´ ê¸¸ì´ë¥´ ê³„ì‚°í•´ sequence_lengthë¡œ ë„˜ê²¨ì£¼ë©´ ëœë‹¤.

í•˜ì§€ë§Œ ì½”ë“œê°€ ë³µì¡í•´ì§€ë¯€ë¡œ **ì—¬ê¸°ì„œëŠ” ê³ ì • ê¸¸ì´ ë‹¨ì–´ë¥¼ ì‚¬ìš©í•œë‹¤.**

ğŸ’¡ í•™ìŠµì‹œí‚¬ ë°ì´í„°ëŠ” ì˜ë¬¸ìë¡œ êµ¬ì„±ëœ ì„ì˜ì˜ ë‹¨ì–´ë¥¼ ì‚¬ìš©í•  ê²ƒì´ê³  í•œ ê¸€ì í•œê¸€ìë¥¼ í•˜ë‚˜ì˜ ë‹¨ê³„ë¡œ ë³¼ ê²ƒì´ë‹¤.

ê·¸ëŸ¬ë©´ í•œ ê¸€ìê°€ í•œ ë‹¨ê³„ì˜ ì…ë ¥ê°’ì´ ë˜ê³  ì´ ê¸€ì ìˆ˜ê°€ ì „ì²´ ë‹¨ê³„ê°€ ëœë‹¤.

![image](https://user-images.githubusercontent.com/66320010/127316378-2cb4e26f-e491-4423-bab7-6837b436c71a.png)

1ï¸âƒ£ ì…ë ¥ìœ¼ë¡œëŠ” ì•ŒíŒŒë²³ ìˆœì„œì—ì„œ ê° ê¸€ìì— í•´ë‹¹í•˜ëŠ” ì¸ë±ìŠ¤ë¥¼ ì›-í•« ì¸ì½”ë”©ìœ¼ë¡œ í‘œí˜„í•œ ê°’ì„ ì·¨í•œë‹¤. ì´ë¥¼ ìœ„í•´ ì•ŒíŒŒë²³ ê¸€ìë“¤ì„ ë°°ì—´ì— ë„£ê³  í•´ë‹¹ ê¸€ìì˜ ì¸ë±ìŠ¤ë¥¼ êµ¬í•  ìˆ˜ ìˆëŠ” ì—°ê´€ ë°°ì—´(ë”•ì…”ë„ˆë¦¬)ë„ ë§Œë“¤ì–´ë‘”ë‹¤.

    import tensorflow as tf
    import numpy as np
    
    char_arr = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']
    num_dic = {n:i for i, n in enumerate(char_arr)}   # {'a':0 , 'b':1 , ~~ }
    dic_len = len(num_dic)
    
2ï¸âƒ£ ê·¸ë¦¬ê³  í•™ìŠµì— ì‚¬ìš©í•  ë‹¨ì–´ë¥¼ ë°°ì—´ë¡œ ì €ì¥í•œë‹¤.

    seq_data = ['word', 'wood', 'deep', 'dive', 'cold', 'cool', 'load', 'love', 'kiss', 'kind']
    
3ï¸âƒ£ ë‹¨ì–´ë“¤ì„ í•™ìŠµì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•´ì£¼ëŠ” ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë¥¼ ì‘ì„±í•œë‹¤. ì´ í•¨ìˆ˜ëŠ” ë‹¤ìŒ ìˆœì„œë¡œ ë°ì´í„°ë¥¼ ë°˜í™˜í•œë‹¤.

1. ì…ë ¥ê°’ìš©ìœ¼ë¡œ ë‹¨ì–´ì˜ ì²˜ìŒ ì„¸ ê¸€ìì˜ ì•ŒíŒŒë²³ ì¸ë±ìŠ¤ë¥¼ êµ¬í•œ ë°°ì—´ì„ ë§Œë“¤ì–´ì¤€ë‹¤.

    - input = [num_dic[n] for n in seq[:-1]]

2. ì¶œë ¥ê°’ìš©ìœ¼ë¡œ ë§ˆì§€ë§‰ ê¸€ìì˜ ì•ŒíŒŒë²³ ì¸ë±ìŠ¤ë¥¼ êµ¬í•œë‹¤.

    - target = num_dic[seq[-1]]

3. ì…ë ¥ê°’ì„ ì›-í•« ì¸ì½”ë”©ìœ¼ë¡œ ë³€í™˜í•œë‹¤.

    - input_batch.append(np.eye(dic_len)[input])

ì˜ˆë¥¼ ë“¤ì–´ "deep"ëŠ” ì…ë ¥ìœ¼ë¡œ d,e,eë¥¼ ì·¨í•˜ê³  ê° ì•ŒíŒŒë²³ì˜ ì¸ë±ìŠ¤ë¥¼ êµ¬í•´ ë°°ì—´ë¡œ ë§Œë“¤ë©´ [3,4,4]ê°€ ë‚˜ì˜¨ë‹¤.

ê·¸ë¦¬ê³  ì´ë¥¼ ì›-í•« ì¸ì½”ë”©í•˜ë©´ ìµœì¢… ì…ë ¥ê°’ì€ ë‹¤ìŒê³¼ ê°™ì´ ëœë‹¤.

    [[0.  0.  0.  1.  0.  0.  0.  ~~ 0.]
     [0.  0.  0.  0.  1.  0.  0.  ~~ 0.]   
     [0.  0.  0.  0.  1.  0.  0.  ~~ 0.]]
   
ì‹¤ì¸¡ê°’ì€ pì˜ ì¸ë±ìŠ¤ì¸ 15ê°€ ë˜ëŠ”ë° ì‹¤ì¸¡ê°’ì€ ì›-í•« ì¸ì½”ë”©í•˜ì§€ ì•Šê³  15ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•  ê²ƒì´ë‹¤. 

ê·¸ ì´ìœ ëŠ” ì†ì‹¤ í•¨ìˆ˜ë¡œ ì§€ê¸ˆê¹Œì§€ ì‚¬ìš©í•˜ë˜ softmax_cross_entropy_with_logitsê°€ ì•„ë‹Œ sparse_softmax_cross_entropy_with_logitsë¥¼ ì‚¬ìš©í•  ê²ƒì´ê¸° ë•Œë¬¸ì´ë‹¤.

sparse_softmax_cross_entropy_with_logits í•¨ìˆ˜ëŠ” ì‹¤ì¸¡ê°’, ì¦‰ labels ê°’ì— ì›-í•« ì¸ì½”ë”©ì„ ì‚¬ìš©í•˜ì§€ ì•Šì•„ë„ ìë™ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ê³„ì‚°í•´ì¤€ë‹¤.

ì´ë ‡ê²Œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì½”ë“œë¡œ ì‘ì„±í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

    def make_batch(seq_data):
        input_batch = []
        target_batch = []
        
        for seq in seq_data:
            input = [num_dic[n] for n in seq[:-1]]
            target = num_dic[seq[-1]]
            input_batch.append(np.eye(dic_len)[input])  # ì…ë ¥ê°’ì„ ì›-í•« ì¸ì½”ë”©ìœ¼ë¡œ ë³€í™˜
            target_batch.append(target)   # ì›-í•« ì¸ì½”ë”© í•˜ì§€ì•Šê³  ì¸ë±ìŠ¤ ë²ˆí˜¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            
        return input_batch, target_batch

4ï¸âƒ£ ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•˜ëŠ” ë¶€ë¶„ì„ ë§ˆì³¤ìœ¼ë‹ˆ ì‹ ê²½ë§ ëª¨ë¸ì„ êµ¬ì„±í•œë‹¤. ë¨¼ì € ì˜µì…˜ë“¤ì„ ì„¤ì •í•œë‹¤.

    learning_rate = 0.01
    n_hidden = 128
    total_epoch = 30
    
    n_step = 3  # ë‹¨ì–´ ì „ì²´ ì¤‘ ì²˜ìŒ 3ê¸€ìë¥¼ ë‹¨ê³„ì ìœ¼ë¡œ í•™ìŠµí•  ê²ƒì´ë¯€ë¡œ n_stepì€ 3ì´ ë¨
    n_input = n_class = dic_len

 - ì…ë ¥ê°’ê³¼ ì¶œë ¥ê°’ì€ ì•ŒíŒŒë²³ì˜ ì›-í•« ì¸ì½”ë”©ì„ ì‚¬ìš©í•  ê²ƒì´ë¯€ë¡œ ì•ŒíŒŒë²³ ê¸€ìë“¤ì˜ ë°°ì—´ í¬ê¸°ì¸ dic_lenê³¼ ê°™ë‹¤.
 
 - ì—¬ê¸°ì„œ ì£¼ì˜í•  ê²ƒì€ **sparse_softmax_cross_entropy_with_logits í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë”ë¼ë„ ë¹„êµë¥¼ ìœ„í•œ ì˜ˆì¸¡ ëª¨ë¸ì˜ ì¶œë ¥ê°’ì€ ì›-í•« ì¸ì½”ë”©ì„ ì‚¬ìš©í•´ì•¼ í•œë‹¤.** 
 
 - ê·¸ë˜ì„œ n_classê°’ë„ n_inputê°’ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ dic_lenê³¼ í¬ê¸°ê°€ ê°™ë„ë¡ ì„¤ì •í–ˆë‹¤.

â— ì¦‰ sparse_softmax_cross_entropy_with_logits í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ë•Œ **ì‹¤ì¸¡ê°’ì¸ labelsê°’ì€ ì¸ë±ìŠ¤ì˜ ìˆ«ìë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ê³  ì˜ˆì¸¡ ëª¨ë¸ì˜ ì¶œë ¥ê°’ì€ ì¸ë±ìŠ¤ì˜ ì›-í•« ì¸ì½”ë”©ì„ ì‚¬ìš©í•œë‹¤.**

5ï¸âƒ£ ë³¸ê²©ì ìœ¼ë¡œ ì‹ ê²½ë§ ëª¨ë¸ì„ êµ¬ì„±í•´ë³¸ë‹¤.

    X = tf.placeholder(tf.float32, [None, n_step, n_input])
    Y = tf.placeholder(tf.int32, [None])  # ì›-í•« ì¸ì½”ë”©ì´ ì•„ë‹ˆë¼ ì¸ë±ìŠ¤ ìˆ«ìë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ê°’ì´ í•˜ë‚˜ë¿ì¸ 1ì°¨ì› ë°°ì—´ì„ ì…ë ¥ìœ¼ë¡œ ë°›ìŒ. ex) [3] [3] [15] [4] ~
    
    W = tf.Variable(tf.random_normal([n_hidden, n_class]))
    b = tf.Variable(tf.random_normal([n_class])) 

6ï¸âƒ£ ë‹¤ìŒìœ¼ë¡œ ë‘ ê°œì˜ RNN ì…€ì„ ìƒì„±í•œë‹¤. ì—¬ëŸ¬ ì…€ì„ ì¡°í•©í•´ ì‹¬ì¸µ ì‹ ê²½ë§ì„ ë§Œë“¤ê¸° ìœ„í•´ì„œì´ë‹¤. DropoutWrapperí•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ RNNì—ë„ ê³¼ì í•© ë°©ì§€ë¥¼ ìœ„í•œ ë“œë¡­ì•„ì›ƒ ê¸°ë²•ì„ ì‰½ê²Œ ì ìš©í•  ìˆ˜ ìˆë‹¤.

    cell1 = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)
    cell1 = tf.nn.rnn_cell.DropoutWrapper(cell1, output_keep_prob = 0.5)   # ë“œë¡­ì•„ì›ƒ
    cell2 = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)
    
7ï¸âƒ£ ì•ì„œ ë§Œë“  ì…€ë“¤ì„ MultiRNNCell í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¡°í•©í•˜ê³  dynamic_rnní•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¬ì¸µ ìˆœí™˜ ì‹ ê²½ë§, ì¦‰ DeepRNNì„ ë§Œë“ ë‹¤.

    multi_cell = tf.nn.run_cell.MultiRNNCell([cell1,cell2])
    outputs, states = tf.nn.dynamic_rnn(multi_cell, X, dtype = tf.float32)
    
8ï¸âƒ£ ì´ì „ ì˜ˆì œì¸ MNISTì˜ˆì¸¡ ëª¨ë¸ê³¼ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ìµœì¢… ì¶œë ¥ì¸µì„ ë§Œë“ ë‹¤.

    outputs = tf.transpose(outputs,[1,0,2])
    outputs = outputs[-1]
    model = tf.matmul(outputs,W) + b
    
9ï¸âƒ£ ë§ˆì§€ë§‰ìœ¼ë¡œ ì†ì‹¤ í•¨ìˆ˜ë¡œëŠ” sparse_softmax_cross_entropy_with_logitsë¥¼, ìµœì í™” í•¨ìˆ˜ë¡œëŠ” AdamOptimizerë¥¼ ì“°ë„ë¡ ì„¤ì •í•˜ì—¬ ì‹ ê²½ë§ ëª¨ë¸ êµ¬ì„±ì„ ë§ˆë¬´ë¦¬í•œë‹¤.

    cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits = model, labels =Y))
    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)
    
ğŸ”Ÿ êµ¬ì„±í•œ ì‹ ê²½ë§ì„ í•™ìŠµì‹œí‚¨ë‹¤. make_batchí•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ seq_dataì— ì €ì¥í•œ ë‹¨ì–´ë“¤ì„ ì…ë ¥ê°’(ì²˜ìŒ ì„¸ ê¸€ì)ê³¼ ì‹¤ì¸¡ê°’(ë§ˆì§€ë§‰ í•œ ê¸€ì)ìœ¼ë¡œ ë¶„ë¦¬í•˜ê³  ì´ ê°’ë“¤ì„ ìµœì í™” í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•˜ëŠ” ì½”ë“œì— ë„£ì–´ ì‹ ê²½ë§ì„ í•™ìŠµì‹œí‚¨ë‹¤.

    sess = tf.Session()
    sess.run(tf.global_variables_initializer())
    
    input_batch, target_batch = make_batch(seq_data)
    
    for epoch in range(total_epoch):
        _, loss = sess.run([optimizer,cost], feed_dict = {X:input_atch, Y: target_batch})
        print('Epoch:', '%04d' % (epoch+1), 'cost=', '{:.6f}.format(loss))
        
    print('ìµœì í™” ì™„ë£Œ!')
    
1ï¸âƒ£1ï¸âƒ£ ê²°ê³¼ê°’ìœ¼ë¡œ ì˜ˆì¸¡í•œ ë‹¨ì–´ë¥¼ ì •í™•ë„ì™€ í•¨ê»˜ ì¶œë ¥í•œë‹¤.    

    prediction = tf.cast(tf.argmax(model,1), tf.int32))
    prediction_check = tf.equal(prediction,Y)  # ì‹¤ì¸¡ê°’ì€ ì›-í•« ì¸ì½”ë”©ì´ ì•„ë‹Œ ì¸ë±ìŠ¤ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ë¯€ë¡œ YëŠ” ì •ìˆ˜ -> ë”°ë¼ì„œ argmaxë¡œ ë³€í™˜í•œ ì˜ˆì¸¡ê°’ë„ ì •ìˆ˜ë¡œ ë³€í™˜í•´ì¤Œ
    accuracy = tf.reduce_mean(tf.cast(prediction_check, tf.float32))
    
1ï¸âƒ£2ï¸âƒ£ í•™ìŠµì— ì‚¬ìš©í•œ ë‹¨ì–´ë“¤ì„ ë„£ê³  ì˜ˆì¸¡ ëª¨ë¸ì„ ëŒë¦°ë‹¤.

    input_batch, target_batch = make_batch(seq_data)
    predict, accuracy_val = sess.run([prediction, accuracy], feed_dict = {X:input_batch, Y:target_batch})
    
1ï¸âƒ£3ï¸âƒ£ ë§ˆì§€ë§‰ìœ¼ë¡œ ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ê°’ë“¤ì„ ê°€ì§€ê³ , ê°ê°ì˜ ê°’ì— í•´ë‹¹í•˜ëŠ” ì¸ë±ìŠ¤ì˜ ì•ŒíŒŒë²³ì„ ê°€ì ¸ì™€ì„œ ì˜ˆì¸¡í•œ ë‹¨ì–´ë¥¼ ì¶œë ¥í•œë‹¤.

    predict_words = []
    for idx, val in enumerate(seq_data):
        last_char = char_arr[predict[idx]]
        predict_words.append(val[:3] + last_char)
        
    print('\n=== ì˜ˆì¸¡ ê²°ê³¼ ===')
    print('ì…ë ¥ê°’:', [w[:3] +' ' for w in seq_data])
    print('ì˜ˆì¸¡ê°’:', predict_words)
    print('ì •í™•ë„:',accuracy_val)
   
 - ê²°ê³¼ëŠ” ë§¤ìš° ì •í™•í•˜ê²Œ ë‚˜ì˜¨ë‹¤.

## 10.3 Sequence to Sequence

