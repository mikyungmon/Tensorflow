# Chapter 10

ì´ë¯¸ì§€ ì¸ì‹ì— CNNì´ ìˆë‹¤ë©´ ìì—°ì–´ ì¸ì‹ì—ëŠ” **ìˆœí™˜ ì‹ ê²½ë§**ì´ë¼ê³  í•˜ëŠ” RNN(Recurrent Neural Network)ê°€ ìˆë‹¤.

RNNì€ ìƒíƒœê°€ ê³ ì •ëœ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë‹¤ë¥¸ ì‹ ê²½ë§ê³¼ëŠ” ë‹¬ë¦¬ ìì—°ì–´ ì²˜ë¦¬ë‚˜ ìŒì„± ì¸ì‹ì²˜ëŸ¼ **ìˆœì„œê°€ ìˆëŠ” ë°ì´í„°**ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë° ê°•ì ì„ ê°€ì§„ ì‹ ê²½ë§ì´ë‹¤.

ì•ì´ë‚˜ ë’¤ì˜ ì •ë³´ì— ë”°ë¼ ì „ì²´ì˜ ì˜ë¯¸ê°€ ë‹¬ë¼ì§€ê±°ë‚˜ ì•ì˜ ì •ë³´ë¡œ ë‹¤ìŒì— ë‚˜ì˜¬ ì •ë³´ë¥¼ ì¶”ì¸¡í•˜ë ¤ëŠ” ê²½ìš°ì— RNNì„ ì‚¬ìš©í•˜ë©´ ì„±ëŠ¥ ì¢‹ì€ í”„ë¡œê·¸ë¨ì„ ë§Œë“¤ ìˆ˜ ìˆë‹¤.

ì´ë²ˆ ì¥ì—ì„œëŠ” RNNì˜ ê¸°ë³¸ì ì¸ ì‚¬ìš©ë²•ì„ ë°°ìš°ê³  ë§ˆì§€ë§‰ì—ëŠ” Sequence to Sequenceëª¨ë¸ì„ ì´ìš©í•´ ê°„ë‹¨í•œ ë²ˆì—­ í”„ë¡œê·¸ë¨ì„ ë§Œë“ ë‹¤.

## 10.1 MNISTë¥¼ RNNìœ¼ë¡œ

ì•ì—ì„œ ì‚¬ìš©í•´ì˜¨ ì†ê¸€ì”¨ ì´ë¯¸ì§€ë¥¼ RNNë°©ì‹ìœ¼ë¡œ í•™ìŠµí•˜ê³  ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ì„ ë§Œë“¤ì–´ë³´ì.

ê¸°ë³¸ì ì¸ RNN ê°œë…ì€ ë‹¤ìŒ ê·¸ë¦¼ê³¼ ê°™ë‹¤.

(ì‚¬ì§„)

- ì´ ê·¸ë¦¼ì˜ ê°€ìš´ë°ì— ìˆëŠ” í•œ ë©ì–´ë¦¬ì˜ ì‹ ê²½ë§ì„ RNNì—ì„œëŠ” **ì…€**ì´ë¼ê³  í•˜ë©° RNNì€ ì´ ì…€ì„ ì—¬ëŸ¬ê°œ ì¤‘ì²©í•˜ì—¬ ì‹¬ì¸µ ì‹ ê²½ë§ì„ ë§Œë“ ë‹¤.

- ê°„ë‹¨í•˜ê²Œ ë§í•´ ì• ë‹¨ê³„ì—ì„œ í•™ìŠµí•œ ê²°ê³¼ë¥¼ ë‹¤ìŒ ë‹¨ê³„ì˜ í•™ìŠµì—ë„ ì´ìš©í•˜ëŠ” ê²ƒì¸ë°, ì´ëŸ° êµ¬ì¡°ë¡œ ì¸í•´ í•™ìŠµ ë°ì´í„°ë¥¼ ë‹¨ê³„ë³„ë¡œ êµ¬ë¶„í•˜ì—¬ ì…ë ¥í•´ì•¼ í•œë‹¤. ë”°ë¼ì„œ MNISTì˜ ì…ë ¥ê°’ë„ ë‹¨ê³„ë³„ë¡œ ì…ë ¥í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë³€ê²½í•´ì¤˜ì•¼í•œë‹¤.

(ì‚¬ì§„)

- ì‚¬ëŒì€ ê¸€ì”¨ë¥¼ ìœ„ì—ì„œ ì•„ë˜ë¡œ ë‚´ë ¤ê°€ë©´ì„œ ì“°ëŠ” ê²½í–¥ì´ ë§ìœ¼ë‹ˆ ë°ì´í„°ë¥¼ ìœ„ ê·¸ë¦¼ì²˜ëŸ¼ êµ¬ì„±í•œë‹¤.

- MNISTê°€ ê°€ë¡œ,ì„¸ë¡œ 28 * 28í¬ê¸°ì´ë‹ˆ ê°€ë¡œ í•œ ì¤„ì˜ 28í”½ì…€ì„ í•œ ë‹¨ê³„ì˜ ì…ë ¥ê°’ìœ¼ë¡œ ì‚¼ê³  ì„¸ë¡œì¤„ì´ ì´ 28ê°œì´ë¯€ë¡œ 28ë‹¨ê³„ë¥¼ ê±°ì³ ë°ì´í„°ë¥¼ ì…ë ¥ë°›ëŠ” ê°œë…ì´ë‹¤.

1ï¸âƒ£ ì½”ë“œë¥¼ ì‘ì„±í•´ë³¸ë‹¤. ë‹¤ìŒì€ í•™ìŠµì— ì‚¬ìš©í•  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë“¤ê³¼ ë³€ìˆ˜, ì¶œë ¥ì¸µì„ ìœ„í•œ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ ì •ì˜í•œ ë¶€ë¶„ì´ë‹¤.

    import tensorflow as tf
    
    from tensorflow.examples.tutorials.mnist import input_data
    mnist = input_data.read_data_sets("./mnist/data/", one_hot = True)
    
    learning_rate = 0.001
    total_epoch = 30
    batch_size = 128
    
    n_input = 28
    n_step = 28
    n_hidden = 128
    n_class = 10
    
    X = tf.placeholder(tf.float32, [None, n_step, n_input])
    Y = tf.placeholder(tf.float32, [None, n_class])
    
    W = tf.Variable(tf.random_normal([n_hidden,n_class]))
    b = tf.Variable(tf.random_normal([n_class]))
    
 - ê¸°ì¡´ ëª¨ë¸ê³¼ ë‹¤ë¥¸ ì ì€ ì…ë ¥ê°’ Xì— n_stepì´ë¼ëŠ” ì°¨ì›ì„ í•˜ë‚˜ ì¶”ê°€í•œ ë¶€ë¶„ì´ë‹¤. RNNì€ ìˆœì„œê°€ ìˆëŠ” ë°ì´í„°ë¥¼ ë‹¤ë£¨ë¯€ë¡œ í•œ ë²ˆì— ì…ë ¥ ë°›ì„ ê°œìˆ˜ì™€ ì´ ëª‡ ë‹¨ê³„ë¡œ ì´ë¤„ì§„ ë°ì´í„°ë¥¼ ë°›ì„ì§€ ì„¤ì •í•´ì•¼ í•œë‹¤.
 
 - ì¶œë ¥ê°’ì€ MNISTì˜ ë¶„ë¥˜ì¸ 0~9ê¹Œì§€ 10ê°œì˜ ìˆ«ìë¥¼ ì›-í•« ì¸ì½”ë”©ìœ¼ë¡œ í‘œí˜„í•˜ë„ë¡ ë§Œë“¤ì—ˆë‹¤.

2ï¸âƒ£ ê·¸ëŸ° ë‹¤ìŒ n_hiddenê°œì˜ ì¶œë ¥ê°’ì„ ê°–ëŠ” RNNì…€ì„ ìƒì„±í•œë‹¤. RNNì„ ì €ìˆ˜ì¤€ë¶€í„° ì§ì ‘ êµ¬í˜„í•˜ë ¤ë©´ ë‹¤ë¥¸ ì‹ ê²½ë§ë³´ë‹¤ ë³µì¡í•œ ê³„ì‚°ì„ ê±°ì³ì•¼ í•˜ì§€ë§Œ í…ì„œí”Œë¡œë¥¼ ì´ìš©í•˜ë©´ ë‹¤ìŒì²˜ëŸ¼ ë§¤ìš° ê°„ë‹¨í•˜ê²Œ ìƒì„±í•  ìˆ˜ ìˆë‹¤.

    cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)
    
  - RNN ê¸°ë³¸ ì‹ ê²½ë§ì€ ê¸´ ë‹¨ê³„ì˜ ë°ì´í„°ë¥¼ í•™ìŠµí•  ë•Œ ë§¨ ë’¤ì—ì„œëŠ” ë§¨ ì•ì˜ ì •ë³´ë¥¼ ì˜ ê¸°ì–µí•˜ì§€ ëª»í•˜ëŠ” íŠ¹ì„±ì´ ìˆë‹¤. ì´ë¥¼ ë³´ì™„í•˜ê¸° ìœ„í•´ ë‹¤ì–‘í•œ êµ¬ì¡°ê°€ ë§Œë“¤ì–´ì¡Œê³  ê·¸ ì¤‘ ë§ì´ ì‚¬ìš©ë˜ëŠ” ê²ƒì´ **LSTM**ì´ë¼ëŠ” ì‹ ê²½ë§ì´ë‹¤.
  
  - GRUëŠ” LSTMê³¼ ë¹„ìŠ·í•˜ì§€ë§Œ êµ¬ì¡°ê°€ ì¡°ê¸ˆ ë” ê°„ë‹¨í•œ ì‹ ê²½ë§ ì•„í‚¤í…ì²˜ì´ë‹¤.

3ï¸âƒ£ ë‹¤ìŒìœ¼ë¡œ dynamic_rnní•¨ìˆ˜ë¥¼ ì´ìš©í•´ RNN ì‹ ê²½ë§ì„ ì™„ì„±í•œë‹¤.

    outputs, states = tf.nn.dynamic_rnn(cell, X, dtype = tf.float32)
    
 - ì•ì„œ ìƒì„±í•œ RNN ì…€ê³¼ ì…ë ¥ê°’, ê·¸ë¦¬ê³  ì…ë ¥ê°’ì˜ ìë£Œí˜•ì„ ë„£ì–´ì£¼ê¸°ë§Œ í•˜ë©´ ê°„ë‹¨í•˜ê²Œ ì‹ ê²½ë§ ìƒì„±í•  ìˆ˜ ìˆë‹¤.

 - ì•ì˜ RNN ê·¸ë¦¼ì²˜ëŸ¼ í•œ ë‹¨ê³„ë¥¼ í•™ìŠµí•œ ë’¤ ìƒíƒœë¥¼ ì €ì¥í•˜ê³  ê·¸ ìƒíƒœë¥¼ ë‹´ì€ ë‹¨ê³„ì˜ ì…ë ¥ ìƒíƒœë¡œ í•˜ì—¬ ë‹¤ì‹œ í•™ìŠµí•œë‹¤. 
 
 - ì´ë ‡ê²Œ ì£¼ì–´ì§„ ë‹¨ê³„ë§Œí¼ ë°˜ë³µí•˜ì—¬ ìƒíƒœë¥¼ ì „íŒŒí•˜ë©´ì„œ ì¶œë ¥ê°’ì„ ë§Œë“¤ì–´ê°€ëŠ” ê²ƒì´ RNNì˜ ê¸°ë³¸ êµ¬ì¡°ì´ë‹¤.
 
 - í•˜ì§€ë§Œ ë°˜ë³µ ë‹¨ê³„ì—ì„œ ê³ ë ¤í•´ì•¼ í•  ê²ƒì´ ë§ê¸° ë•Œë¬¸ì— ìš°ë¦¬ëŠ” ì´ ê³¼ì •ì„ ëŒ€ì‹  í•´ì£¼ëŠ” dynamic_rnní•¨ìˆ˜ë¥¼ ì‚¬ìš©í–ˆë‹¤. ì´ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë©´ ë‹¨ ë‘ì¤„ë¡œ RNN ëª¨ë¸ì˜ í•µì‹¬ êµ¬ì¡°(ì…€ê³¼ ì‹ ê²½ë§)ë¥¼ ë§Œë“¤ ìˆ˜ ìˆë‹¤.

4ï¸âƒ£ RNNì—ì„œ ë‚˜ì˜¨ ì¶œë ¥ê°’ì„ ê°€ì§€ê³  ìµœì¢… ì¶œë ¥ê°’ì„ ë§Œë“¤ì–´ë³¸ë‹¤.

ê²°ê³¼ê°’ì„ ì›-í•« ì¸ì½”ë”© í˜•íƒœë¡œ ë§Œë“¤ ê²ƒì´ê¸° ë•Œë¬¸ì— ì†ì‹¤ í•¨ìˆ˜ë¡œ tf.nn.softmax_cross_entropy_with_logitsì„ ì‚¬ìš©í•œë‹¤. 

ì´ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ ìµœì¢… ê²°ê³¼ê°’ì´ ì‹¤ì¸¡ê°’ Yì™€ ë™ì¼í•œ í˜•íƒœì¸ [batch_size, n_class]ì—¬ì•¼ í•œë‹¤. 

ì•ì—ì„œ ì´ í˜•íƒœì˜ ì¶œë ¥ê°’ì„ ë§Œë“¤ê¸° ìœ„í•´ì„œ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ ë‹¤ìŒê³¼ ê°™ì´ ì„¤ì •í–ˆë‹¤.

    W = tf.Variable(tf.random_normal([n_hidden,n_class]))
    b = tf.Variable(tf.random_normal([n_class]))
    
ê·¸ëŸ°ë° RNN ì‹ ê²½ë§ì—ì„œ ë‚˜ì˜¤ëŠ” ì¶œë ¥ê°’ì€ ê° ë‹¨ê³„ê°€ í¬í•¨ëœ **[batch_size, n_step, n_hidden] í˜•íƒœ**ì´ë‹¤.

ë”°ë¼ì„œ ì€ë‹‰ì¸µì˜ ì¶œë ¥ê°’ì„ ê°€ì¤‘ì¹˜ Wì™€ ê°™ì€ í˜•íƒœë¡œ ë§Œë“¤ì–´ì¤˜ì•¼ í–‰ë ¬ê³±ì„ ìˆ˜í–‰í•˜ì—¬ ì›í•˜ëŠ” ì¶œë ¥ê°’ì„ ì–»ì„ ìˆ˜ ìˆë‹¤.

(ì°¸ê³ ë¡œ dynamic_rnn í•¨ìˆ˜ì˜ ì˜µì…˜ ì¤‘ time_majorì˜ ê°’ì„ Trueë¡œ í•˜ë©´ [batch_size, n_step, n_hidden] í˜•íƒœë¡œ ì¶œë ¥ëœë‹¤.)

    # outputs : [batch_size, n_step, n_hidden] -> [ n_step, batch_size, n_hidden]
    outputs = tf.transpose(outputs,[1,0,2])   # n_stepê³¼ batch_size ì°¨ì›ì˜ ìˆœì„œë¥¼ ë°”ê¿ˆ
    # outputs : [batch_size, n_hidden]
    outputs = outputs[-1]  # n_step ì°¨ì› ì œê±° -> ë§ˆì§€ë§‰ ë‹¨ê³„ì˜ ê²°ê³¼ê°’ë§Œ ì·¨í•¨
    
5ï¸âƒ£ ì´ì œ ì¸ê³µì‹ ê²½ë§ì˜ ê¸°ë³¸ ìˆ˜ì‹ì´ì í•µì‹¬ì¸ y = X * W + bë¥¼ ì´ìš©í•˜ì—¬ ìµœì¢… ê²°ê³¼ê°’ì„ ë§Œë“ ë‹¤.

    model = tf.matmul(outputs,W) + b
    
6ï¸âƒ£ ì§€ê¸ˆê¹Œì§€ ë§Œë“  ëª¨ë¸ê³¼ ì‹¤ì¸¡ê°’ì„ ë¹„êµí•˜ì—¬ ì†ì‹¤ê°’ì„ êµ¬í•˜ê³  ì‹ ê²½ë§ì„ ìµœì í™”í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹ ê²½ë§ êµ¬ì„±ì„ ë§ˆë¬´ë¦¬í•œë‹¤.

    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = model, label = Y))
    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)

7ï¸âƒ£ ì•ì„œ êµ¬ì„±í•œ ì‹ ê²½ë§ì„ í•™ìŠµì‹œí‚¤ê³  ê²°ê³¼ë¥¼ í™•ì¸í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•œë‹¤. ì• ì¥ì˜ ì½”ë“œì™€ ê±°ì˜ ê°™ì§€ë§Œ ì…ë ¥ê°’ì´ [batch_size, n_step,n_input]í˜•íƒœì´ë¯€ë¡œ CNNì—ì„œ ì‚¬ìš©í•œ ê²ƒì²˜ëŸ¼ reshapeí•¨ìˆ˜ë¥¼ ì´ìš©í•´ ë°ì´í„° í˜•íƒœë¥¼ ë°”ê¿”ì£¼ëŠ” ë¶€ë¶„ë§Œ ì£¼ì˜í•˜ë©´ ëœë‹¤.

    sess = tf.Session()
    sess.run(tf.global_variables_initializer())
    
    total_batch = int(mnist.train.num_exmples / batch_size)
    
    for epoch in range(total_epoch):
        total_cost = 0
        
        for i in range(total_batch):
            batch_xs, batch_ys = mnist.train.next_batch(batch_size)
            batch_xs = batch_xs.reshape((batch_size,n_step, n_input))
        
            _, cost_val = sess.run([optimizer,cost], feed_dict = {X:batch_xs, Y: batch_ys})
        
            total_cost += cost_val
        
        print('Epoch:', '%04d' %(epoch+1), 'Avg. cost = ', '{:.3f}'.format(total_cost / total_batch))
      
    print('ìµœì í™” ì™„ë£Œ!')
    
    is_correct = tf.equal(tf.argmax(model,1), tf.argmax(Y,1))
    accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))
    
    test_batch_size = len(mnist.test.images)
    test_xs = mnist.test.images.reshape(test_batch_size, n_step, n_input)
    test_ys = mnist.test.labels
    
    print('ì •í™•ë„:', sess.run(accuracy, feed_dict = {X:test_xs, Y: test_ys}))
    
## 10.2 ë‹¨ì–´ ìë™ ì™„ì„±

ì´ë²ˆì—ëŠ” RNN ëª¨ë¸ì„ ì´ìš©í•˜ì—¬ ë‹¨ì–´ë¥¼ ìë™ ì™„ì„±í•˜ëŠ” í”„ë¡œê·¸ë¨ì„ ë§Œë“¤ì–´ë³¸ë‹¤.

ì˜ë¬¸ì 4ê°œë¡œ êµ¬ì„±ëœ ë‹¨ì–´ë¥¼ í•™ìŠµì‹œì¼œ 3ê¸€ìë§Œ ì£¼ì–´ì§€ë©´ ë‚˜ë¨¸ì§€ í•œ ê¸€ìë¥¼ ì¶”ì²œí•˜ì—¬ ë‹¨ì–´ë¥¼ ì™„ì„±í•˜ëŠ” í”„ë¡œê·¸ë¨ì´ë‹¤.

ì°¸ê³ ë¡œ dynamic_rnnì˜ sequence_lengthì˜µì…˜ì„ ì‚¬ìš©í•˜ë©´ ê°€ë³€ ê¸¸ì´ ë‹¨ì–´ë¥¼ í•™ìŠµì‹œí‚¬ ìˆ˜ ìˆë‹¤. 

ì§§ì€ ë‹¨ì–´ëŠ” ê°€ì¥ ê¸´ ë‹¨ì–´ì˜ ê¸¸ì´ ë§Œí¼ ë’·ë¶€ë¶„ì„ 0ìœ¼ë¡œ ì±„ìš°ê³  í•´ë‹¹ ë‹¨ì–´ ê¸¸ì´ë¥´ ê³„ì‚°í•´ sequence_lengthë¡œ ë„˜ê²¨ì£¼ë©´ ëœë‹¤.

í•˜ì§€ë§Œ ì½”ë“œê°€ ë³µì¡í•´ì§€ë¯€ë¡œ **ì—¬ê¸°ì„œëŠ” ê³ ì • ê¸¸ì´ ë‹¨ì–´ë¥¼ ì‚¬ìš©í•œë‹¤.**

ğŸ’¡ í•™ìŠµì‹œí‚¬ ë°ì´í„°ëŠ” ì˜ë¬¸ìë¡œ êµ¬ì„±ëœ ì„ì˜ì˜ ë‹¨ì–´ë¥¼ ì‚¬ìš©í•  ê²ƒì´ê³  í•œ ê¸€ì í•œê¸€ìë¥¼ í•˜ë‚˜ì˜ ë‹¨ê³„ë¡œ ë³¼ ê²ƒì´ë‹¤.

ê·¸ëŸ¬ë©´ í•œ ê¸€ìê°€ í•œ ë‹¨ê³„ì˜ ì…ë ¥ê°’ì´ ë˜ê³  ì´ ê¸€ì ìˆ˜ê°€ ì „ì²´ ë‹¨ê³„ê°€ ëœë‹¤.

(ì‚¬ì§„)

1ï¸âƒ£












