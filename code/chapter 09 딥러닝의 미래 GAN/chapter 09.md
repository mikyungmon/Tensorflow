# Chapter 09

GANì€ ì˜¤í† ì¸ì½”ë”ì™€ ê°™ì´ ê²°ê³¼ë¬¼ì„ ìƒì„±í•˜ëŠ” ìƒì„± ëª¨ë¸ ì¤‘ í•˜ë‚˜ë¡œ ì„œë¡œ ëŒ€ë¦½í•˜ëŠ” ë‘ ì‹ ê²½ë§ì„ ê²½ìŸì‹œì¼œê°€ë©° ê²°ê³¼ë¬¼ ìƒì„± ë°©ë²•ì„ í•™ìŠµí•œë‹¤.

ì´í•´ë¥¼ ë•ê¸° ìœ„í•´ ì ì ˆí•œ ë¹„ìœ ë¥¼ ë“¤ì–´ë³´ê² ë‹¤.

ìœ„ì¡°ì§€íë²”(ìƒì„±ì)ê³¼ ê²½ì°°(êµ¬ë¶„ì)ì— ëŒ€í•œ ì´ì•¼ê¸°ë¡œ, ìœ„ì¡°ì§€íë²”ì€ ê²½ì°°ì„ ìµœëŒ€í•œ ì†ì´ë ¤ê³  í•˜ê³  ê²½ì°°ì€ ìœ„ì¡°í•œ ì§€íë¥¼ ìµœëŒ€í•œ ê°ë³„í•˜ë ¤ê³  ë…¸ë ¥í•œë‹¤.

ì´ì²˜ëŸ¼ ìœ„ì¡°ì§€íë¥¼ ë§Œë“¤ê³  ê°ë³„í•˜ë ¤ëŠ” ê²½ìŸì„ í†µí•´ ì„œë¡œì˜ ëŠ¥ë ¥ì´ ë°œì „í•˜ê²Œ ë˜ê³  ê·¸ëŸ¬ë‹¤ ë³´ë©´ ê²°êµ­ ìœ„ì¡°ì§€íë²”ì€ ì§„ì§œì™€ ê±°ì˜ êµ¬ë¶„í•  ìˆ˜ ì—†ì„ ì •ë„ë¡œ ì§„ì§œ ê°™ì€ ìœ„ì¡°ì§€íë¥¼ ë§Œë“¤ ìˆ˜ ìˆê²Œ ëœë‹¤ëŠ” ê²ƒì´ë‹¤.

ë¨¼ì € ì‹¤ì œ ì´ë¯¸ì§€ë¥¼ ì£¼ê³  **êµ¬ë¶„ì**ì—ê²Œ ì´ ì´ë¯¸ì§€ê°€ ì§„ì§œì„ì„ íŒë‹¨í•˜ê²Œ í•œë‹¤. ê·¸ ë‹¤ìŒ **ìƒì„±ì**ë¥¼ í†µí•´ ë…¸ì´ì¦ˆë¡œë¶€í„° ì„ì˜ì˜ ì´ë¯¸ì§€ë¥¼ ë§Œë“¤ê³  ì´ê²ƒì„ ë‹¤ì‹œ ê°™ì€ êµ¬ë¶„ìë¥¼ í†µí•´ ì§„ì§œ ì´ë¯¸ì§€ì¸ì§€ë¥¼ íŒë‹¨í•˜ê²Œ í•œë‹¤.

ğŸ’¡ ì´ë ‡ê²Œ ìƒì„±ìëŠ” êµ¬ë¶„ìë¥¼ ì†ì—¬ ì§„ì§œì²˜ëŸ¼ ë³´ì´ê²Œ í•˜ê³ , êµ¬ë¶„ìëŠ” ìƒì„±ìê°€ ë§Œë“  ì´ë¯¸ì§€ë¥¼ ìµœëŒ€í•œ ê°€ì§œë¼ê³  êµ¬ë¶„í•˜ë„ë¡ í›ˆë ¨í•˜ëŠ” ê²ƒì´ GANì˜ í•µì‹¬ì´ë‹¤. ìƒì„±ìì™€ êµ¬ë¶„ìì˜ ê²½ìŸì„ í†µí•´ ê²°ê³¼ì ìœ¼ë¡œëŠ” ìƒì„±ìëŠ” ì‹¤ì œ ì´ë¯¸ì§€ì™€ ìƒë‹¹íˆ ë¹„ìŠ·í•œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•´ë‚¼ ìˆ˜ ìˆê²Œ ëœë‹¤.

ì´ë²ˆ ì¥ì—ì„œëŠ” ì´ GAN ëª¨ë¸ì„ í™œìš©í•˜ì—¬ MNISTì†ê¸€ì”¨ ìˆ«ìë¥¼ ë¬´ì‘ìœ„ë¡œ ìƒì„±í•˜ëŠ” ê°„ë‹¨í•œ ì˜ˆì œë¥¼ ë§Œë“¤ì–´ë³´ê³  ëª¨ë¸ì„ í™•ì¥í•˜ì—¬ ì›í•˜ëŠ” ìˆ«ìì— í•´ë‹¹í•˜ëŠ” ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” ëª¨ë¸ì„ ë§Œë“¤ì–´ ë³¼ ê²ƒì´ë‹¤.

## 9.1 GAN ê¸°ë³¸ ëª¨ë¸ êµ¬í˜„í•˜ê¸°

1. ë¨¼ì € í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë¶ˆëŸ¬ë“¤ì¸ë‹¤. ìƒì„±ëœ ì´ë¯¸ì§€ë“¤ì„ ë³´ì—¬ì¤„ ê²ƒì´ë¯€ë¡œ matplotlibê³¼ numpyë„ ê°™ì´ ì„í¬íŠ¸í•œë‹¤.

       import tensorflow as tf
       import matplotlib.pyplot as plt
       import numpy as np
       
       from tensorflow.examples.tutorials.mnist import input_data
       mnist = input_data.read_data_sets("./mnist/data/", one_hot = True)
       
2. ë‹¤ìŒìœ¼ë¡œëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì„¤ì •í•œë‹¤.

       total_epoch  = 100
       batch_size = 100
       learning_rate = 0.0002
       h_hidden = 256
       n_input = 28 * 28
       n_noise = 128    # ìƒì„±ìì˜ ì…ë ¥ê°’ìœ¼ë¡œ ì‚¬ìš©í•  ë…¸ì´ì¦ˆì˜ í¬ê¸°

3. í”Œë ˆì´ìŠ¤í™€ë”ë¥¼ ì„¤ì •í•œë‹¤. GANë„ ë¹„ì§€ë„ í•™ìŠµì´ë¯€ë¡œ Yë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤. ë‹¤ë§Œ êµ¬ë¶„ìì— ë„£ì„ ì´ë¯¸ì§€ê°€ ì‹¤ì œ ì´ë¯¸ì§€ì™€ ìƒì„±í•œ ê°€ì§œ ì´ë¯¸ì§€ ë‘ ê°œì´ê³ , ê°€ì§œ ì´ë¯¸ì§€ëŠ” ë…¸ì´ì¦ˆì—ì„œ ìƒì„±í•  ê²ƒì´ë¯€ë¡œ ë…¸ì´ì¦ˆë¥¼ ì…ë ¥í•  í”Œë ˆì´ìŠ¤í™€ë” Zë¥¼ ì¶”ê°€í•œë‹¤.

       X = tf.placeholder(tf.float32, [None,n_input])
       Z = tf.placeholder(tf.float32, [None,n_noise])
      
4. ìƒì„±ì ì‹ ê²½ë§ì— ì‚¬ìš©í•  ë³€ìˆ˜ë“¤ì„ ì„¤ì •í•œë‹¤. 

       # ì²« ë²ˆì§¸ ê°€ì¤‘ì¹˜, í¸í–¥ -> ì€ë‹‰ì¸µìœ¼ë¡œ ì¶œë ¥í•˜ê¸° ìœ„í•œ ë³€ìˆ˜ë“¤
       G_W1 = tf.Variable(tf.random_normal([n_noise, n_hidden], stddev = 0.01))
       G_b1 = tf.Variable(tf.zeros([n_hidden])
       # ë‘ ë²ˆì§¸ ê°€ì¤‘ì¹˜, í¸í–¥ -> ì¶œë ¥ì¸µì— ì‚¬ìš©í•  ë³€ìˆ˜ë“¤
       G_W2 = tf.Variable(tf.random_normal([n_hidden, n_input], stddev = 0.01))
       G_b2 = tf.Variable(tf.zeros([n_input])

5. êµ¬ë¶„ì ìƒì„±ë§ì— ì‚¬ìš©í•  ë³€ìˆ˜ë“¤ì„ ì„¤ì •í•œë‹¤. ì€ë‹‰ì¸µì€ ìƒì„±ìì™€ ë™ì¼í•˜ê²Œ êµ¬ì„±í•œë‹¤. 

       D_W1 = tf.Variable(tf.random_normal([n_input, n_hidden], stddev = 0.01))
       D_b1 = tf.Variable(tf.zeros([n_hidden]))
       D_W2 = tf.Variable(tf.random_normal([n_hidden, 1], stddev = 0.01))
       D_b2 = tf.Variable(tf.zeros([1]))       

â— **ì‹¤ì œ ì´ë¯¸ì§€ë¥¼ íŒë³„í•˜ëŠ” êµ¬ë¶„ì ì‹ ê²½ë§ê³¼ ìƒì„±í•œ ì´ë¯¸ì§€ë¥¼ íŒë³„í•˜ëŠ” êµ¬ë¶„ì ì‹ ê²½ë§ì„ ê°™ì€ ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•´ì•¼í•œë‹¤. ê°™ì€ ì‹ ê²½ë§ìœ¼ë¡œ êµ¬ë¶„ì„ ì‹œì¼œì•¼ ì§„ì§œì™€ ê°€ì§œ ì´ë¯¸ì§€ë¥¼ êµ¬ë¶„í•˜ëŠ” íŠ¹ì§•ë“¤ì„ ë™ì‹œì— ì¡ì•„ë‚¼ ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤.**

6. ë¨¼ì € ìƒì„±ì ì‹ ê²½ë§ì„ êµ¬í˜„í•´ë³¸ë‹¤.

       def generator(noise_z):
         hidden = tf.nn.relu(tf.matmul(noise_z, G_W1) + b1)
         output = tf.nn.sigmoid(tf.matmul(hidden, G_W2) + b2)

         return output

    - ìƒì„±ìëŠ” ë¬´ì‘ìœ„ë¡œ ìƒì„±í•œ ë…¸ì´ì¦ˆë¥¼ ë°›ì•„ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ ë°˜ì˜í•˜ì—¬ ì€ë‹‰ì¸µì„ ë§Œë“¤ê³  ì€ë‹‰ì¸µì—ì„œ ì‹¤ì œ ì´ë¯¸ì§€ì™€ ê°™ì€ í¬ê¸°ì˜ ê²°ê³¼ê°’ì„ ì¶œë ¥í•˜ëŠ” ê°„ë‹¨í•œ êµ¬ì„±ì´ë‹¤.

7. êµ¬ë¶„ì ì‹ ê²½ë§ ì—­ì‹œ ê°™ì€ êµ¬ì„±ì´ì§€ë§Œ 0~1ì‚¬ì´ì˜ ìŠ¤ì¹¼ë¼ê°’ì„ í•˜ë‚˜ ì¶œë ¥í•˜ë„ë¡ í•˜ì˜€ê³  ì´ë¥¼ ìœ„í•œ í™œì„±í™” í•¨ìˆ˜ë¡œ sigmoidí•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œë‹¤.

       def discriminator(inputs):
          hidden = tf.nn.relu(tf.matmul(inputs,D_W1) + D_b1)
          output = tf.nn.sigmoid(tf.matmul(hidden,D_W2) + D_b2)
          
          return output
          
8. ë¬´ì‘ìœ„í•œ ë…¸ì´ì¦ˆë¥¼ ë§Œë“¤ì–´ì£¼ëŠ” ê°„ë‹¨í•œ í•¨ìˆ˜ë¥¼ ë§Œë“ ë‹¤.

       def get_noise(batch_size, n_noise):
          return np.random_normal(size = (batch_size, n_noise))
          
9. ë§ˆì§€ë§‰ìœ¼ë¡œ ë…¸ì´ì¦ˆ Zë¥¼ ì´ìš©í•´ ê°€ì§œ ì´ë¯¸ì§€ë¥¼ ë§Œë“¤ ìƒì„±ì Gë¥¼ ë§Œë“¤ê³  ì´ Gê°€ ë§Œë“  ê°€ì§œ ì´ë¯¸ì§€ì™€ ì§„ì§œ ì´ë¯¸ì§€ Xë¥¼ ê°ê° êµ¬ë¶„ìì— ë„£ì–´ ì…ë ¥í•œ ì´ë¯¸ì§€ê°€ ì§„ì§œì¸ì§€ë¥¼ íŒë³„í•˜ë„ë¡ í•œë‹¤.

       G = generator(Z)
       D_gene = discriminator(G)
       D_real = discriminator(X)

10. ë‹¤ìŒìœ¼ë¡œëŠ” ì†ì‹¤ê°’ì„ êµ¬í•´ì•¼í•˜ëŠ”ë° ì´ë²ˆì—ëŠ” ë‘ ê°œê°€ í•„ìš”í•˜ë‹¤. ìƒì„±ìê°€ ë§Œë“  ì´ë¯¸ì§€ë¥¼ êµ¬ë¶„ìê°€ ê°€ì§œë¼ê³  íŒë‹¨í•˜ë„ë¡ í•˜ëŠ” ì†ì‹¤ê°’(ê²½ì°° í•™ìŠµìš©)ê³¼ ì§„ì§œë¼ê³  íŒë‹¨í•˜ë„ë¡ í•˜ëŠ” ì†ì‹¤ê°’(ìœ„ì¡°ì§€íë²” í•™ìŠµìš©)ì„ êµ¬í•´ì•¼í•œë‹¤. ê²½ì°°ì„ í•™ìŠµì‹œí‚¤ë ¤ë©´ ì§„ì§œ ì´ë¯¸ì§€ íŒë³„ê°’ D_realì€ 1ì— ê°€ê¹Œì›Œì•¼í•˜ê³  ê°€ì§œ ì´ë¯¸ì§€ íŒë³„ê°’ D_geneì€ 0ì— ê°€ê¹Œì›Œì•¼í•œë‹¤.

        loss_D = tf.reduce_mean(tf.log(D_real) + tf.log(1-D_gene))   # ì´ ê°’ì„ ìµœëŒ€í™”í•˜ë©´ ê²½ì°° í•™ìŠµì´ ì´ë£¨ì–´ì§

ë‹¤ìŒìœ¼ë¡œ ìœ„ì¡°ì§€íë²” í•™ìŠµì€ íŒë³„ê°’ D_geneë¥¼ 1ì— ê°€ê¹ê²Œ ë§Œë“¤ê¸°ë§Œ í•˜ë©´ëœë‹¤. ì¦‰ ê°€ì§œ ì´ë¯¸ì§€ë¥¼ ë„£ì–´ë„ ì§„ì§œ ê°™ë‹¤ê³  íŒë³„í•´ì•¼ í•œë‹¤. ë‹¤ìŒê³¼ ê°™ì´ D_geneë¥¼ ê·¸ëŒ€ë¡œ ë„£ì–´ ì´ë¥¼ ì†ì‹¤ê°’ìœ¼ë¡œ í•˜ê³  ì´ ê°’ì„ ìµœëŒ€í™”í•˜ë©´ ìœ„ì¡°ì§€íë²”ì„ í•™ìŠµì‹œí‚¬ ìˆ˜ ìˆë‹¤.

    loss G = tf.reduce_mean(tf.log(D_gene))
    
âœ” ì¦‰ GANì˜ í•™ìŠµì€ loss_Dì™€ loss_G ëª¨ë‘ë¥¼ ìµœëŒ€í™”í•˜ëŠ” ê²ƒì´ë‹¤. ë‹¤ë§Œ loss_Dì™€ loss_GëŠ” ì„œë¡œ ì—°ê´€ë˜ì–´ ìˆì–´ì„œ ë‘ ì†ì‹¤ê°’ì´ í•­ìƒ ê°™ì´ ì¦ê°€í•˜ëŠ” ê²½í–¥ì„ ë³´ì´ì§€ëŠ” ì•Šì„ ê²ƒì´ë‹¤. loss_Dê°€ ì¦ê°€í•˜ë ¤ë©´ loss_GëŠ” í•˜ë½í•´ì•¼í•˜ê³  ë°˜ëŒ€ë¡œ loss_Gê°€ ì¦ê°€í•˜ë ¤ë©´ loss_DëŠ” í•˜ë½í•´ì•¼í•˜ëŠ” ê²½ìŸ ê´€ê³„ì´ê¸° ë•Œë¬¸ì´ë‹¤.


11. ì´ì œ ì´ ì†ì‹¤ê°’ë“¤ì„ ì´ìš©í•´ í•™ìŠµì‹œí‚¤ëŠ” ì¼ë§Œ ë‚¨ì•˜ë‹¤. ì´ ë•Œ ì£¼ì˜í•  ì ì€ loss_Dë¥¼ êµ¬í•  ë•Œ êµ¬ë¶„ì ì‹ ê²½ë§ì— ì‚¬ìš©ë˜ëŠ” ë³€ìˆ˜ë“¤ë§Œ ì‚¬ìš©í•˜ê³  loss_Gë¥¼ êµ¬í•  ë•ŒëŠ” ìƒì„±ì ì‹ ê²½ë§ì— ì‚¬ìš©ë˜ëŠ” ë³€ìˆ˜ë“¤ë§Œ ì‚¬ìš©í•˜ì—¬ ìµœì í™”í•´ì•¼ í•œë‹¤. ê·¸ë˜ì•¼ loss_Dë¥¼ í•™ìŠµí•  ë•ŒëŠ” ìƒì„±ìê°€ ë³€í•˜ì§€ ì•Šê³  loss_Gë¥¼ í•™ìŠµí•  ë•ŒëŠ” êµ¬ë¶„ìê°€ ë³€í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì´ë‹¤.

        D_var_list = [D_W1, D_b1, D_W2, D_b2]
        G_var_list = [G_W1, G_b1, G_W2, G_b2]
        
        train_D = tf.train.AdamOptimizer(learning_rate).minimize(-loss_D, var_list = D_var_list)   # lossë¥¼ ìµœëŒ€í™”í•´ì•¼ í•˜ì§€ë§Œ ìµœì í™”ì— ì“¸ ìˆ˜ ìˆëŠ” í•¨ìˆ˜ëŠ” minimizeë¿ì´ë¯€ë¡œ ìµœì í™”í•˜ë ¤ëŠ” loss_D ì•ì— ìŒìˆ˜ ë¶€í˜¸ ë¶™ì—¬ì¤Œ
        train_G = tf.train.AdamOptimizer(learning_rate).minimize(-loss_D, var_list = D_var_list)

12. í•™ìŠµì„ ì‹œí‚¤ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•œë‹¤. ì§€ê¸ˆê¹Œì§€ ë³¸ í•™ìŠµ ì½”ë“œì™€ ê±°ì˜ ê°™ì§€ë§Œ ì´ë²ˆ ëª¨ë¸ì—ì„œëŠ” ë‘ ê°œì˜ ì†ì‹¤ê°’ì„ í•™ìŠµì‹œì¼œì•¼ í•´ì„œ ì½”ë“œê°€ ì•½ê°„ ì¶”ê°€ë˜ì—ˆë‹¤.

        sess = tf.Session()
        sess.run(tf.global_variables_initialier())
        
        total_batch = int(mnist.train.num_examples / batch_size)
        loss_val_D , loss_val_G = 0, 0   # loss_Dì™€ loss_Gì˜ ê²°ê³¼ê°’ì„ ë°›ì„ ë³€ìˆ˜
        
        for epoch in range(total_epoch):
            for i in range(total_batch):
                batch_xs, batch_ys = mnist.train.next_batch(batch_size)
                noise = get_noise(batch_size, n_noise)   # ë°°ì¹˜ í¬ê¸°ë§Œí¼ ë…¸ì´ì¦ˆ ìƒì„±
                
                _, loss_val_D = sess.run([train_D, loss_D], feed_dict = {X: batch_xs, Y:noise})
                _. loss_val_G = sess.run([train_G, loss_G], feed_dict = {Z:noise})
                
            print('Epoch:', '%04d' % epoch, 'D loss: {:.4}'.format(loss_val_D), 'G loss: {:.4}'.format(loss_val_G)
            
13. ëª¨ë¸ì„ ì™„ì„±í•˜ì˜€ìœ¼ë‹ˆ ì´ì œ í•™ìŠµ ê²°ê³¼ë¥¼ í™•ì¸í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•´ë³¸ë‹¤. í•™ìŠµì´ ì˜ ë˜ëŠ”ì§€ëŠ” 0,9,19,29ë²ˆì§¸,, ~ ë§ˆë‹¤ ìƒì„±ê¸°ë¡œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ì—¬ ëˆˆìœ¼ë¡œ ì§ì ‘ í™•ì¸í•˜ë„ë¡í•œë‹¤. ê²°ê³¼ë¥¼ í™•ì¸í•˜ëŠ” ì½”ë“œëŠ” í•™ìŠµ ë£¨í”„ ì•ˆì— ì‘ì„±í•´ì•¼í•œë‹¤.
            
            
        # ë…¸ì´ì¦ˆë¥¼ ë§Œë“¤ê³  ìƒì„±ì Gì— ë„£ì–´ ê²°ê³¼ê°’ì„ ë§Œë“¦    
        if epoch == 0 or (epoch+1) % 10 == 0:
            sample_size = 10
            noise = get_noise(sample_size,n_noise)
            samples = sess.run(G, feed_dict = {Z:noise})   # samplesí´ë”ëŠ” ë¯¸ë¦¬ ë§Œë“¤ì–´ì ¸ ìˆì–´ì•¼ í•¨
            
            # ì´ ê²°ê³¼ê°’ë“¤ì„ 28 * 28 í¬ê¸°ì˜ ê°€ì§œ ì´ë¯¸ì§€ë¡œ ë§Œë“¤ì–´ samplesí´ë”ì— ì €ì¥í•˜ë„ë¡í•¨
            fix, ax = plt.subplots(1,sample_size, figsize = (sample_size,1))
            
            for i in range(sample_size):
                ax[i].set.axis_off()
                ax[i].imshow(np.reshape(sample[i], (28,28)))
                
            plt.savefig('samples/{}.png'.format(str(epoch).zfill(3)), bbox_inches='tight')
            plt.close(fig)
            
        print('ìµœì í™” ì™„ë£Œ!')
              
í•™ìŠµì´ ì •ìƒì ìœ¼ë¡œ ì§„í–‰ë˜ì—ˆë‹¤ë©´ í•™ìŠµ ì„¸ëŒ€ê°€ ì§€ë‚˜ë©´ì„œ ì´ë¯¸ì§€ê°€ ì ì  ë” ê·¸ëŸ´ë“¯í•´ì§€ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.

## 9.2 ì›í•˜ëŠ” ìˆ«ì ìƒì„±í•˜ê¸°

ì´ë²ˆì—ëŠ” ìˆ«ìë¥¼ ë¬´ì‘ìœ„ë¡œ ìƒì„±í•˜ì§€ ì•Šê³  ì›í•˜ëŠ” ìˆ«ìë¥¼ ì§€ì •í•´ ìƒì„±í•˜ëŠ” ëª¨ë¸ì„ ë§Œë“¤ì–´ë³¼ ê²ƒì´ë‹¤.

1. ê°„ë‹¨í•˜ê²Œ ë…¸ì´ì¦ˆì— ë ˆì´ë¸” ë°ì´í„°ë¥¼ íŒíŠ¸ë¡œ ë„£ì–´ ì£¼ëŠ” ë°©ë²•ì„ ì‚¬ìš©í•œë‹¤.

       import tensorflow as tf
       import matplotlib.pyplot as plt
       import numpy as np
       
       from tensorflow.examples.tutorials.mnist import input_data
       mnist = input_data.read_data_set("./mnist/data/", one_hot = True)
       
       total_epoch = 100
       batch_size = 100
       h_hidden = 256
       n_input = 28 * 28
       n_noise = 128
       n_class = 10
       
       X = tf.placeholder(tf.float32, [None,n_input])
       Y = tf.placeholder(tf.float32, [None,n_class])  # ê²°ê³¼ê°’ íŒì •ìš©ì€ ì•„ë‹ˆê³  ë…¸ì´ì¦ˆì™€ ì‹¤ì œ ì´ë¯¸ì§€ì— ê°ê° í•´ë‹¹í•˜ëŠ” ìˆ«ìë¥¼ íŒíŠ¸ë¡œ ë„£ì–´ì£¼ëŠ” ìš©ë„
       Z = tf.placeholder(tf.float32, [None,n_noise])

2. ìƒì„±ì ì‹ ê²½ë§ì„ êµ¬ì„±í•´ë³¼ê±´ë° ì—¬ê¸°ì„œëŠ” ë³€ìˆ˜ë“¤ì„ ì„ ì–¸í•˜ì§€ ì•Šê³  tf.layersë¥¼ ì‚¬ìš©í•œë‹¤. ì•ì„œ ë³¸ ê²ƒì²˜ëŸ¼ GAN ì€ ìƒì„±ìì™€ êµ¬ë¶„ìë¥¼ ë™ì‹œì— í•™ìŠµì‹œì¼œì•¼í•˜ê³  ë”°ë¼ì„œ í•™ìŠµ ì‹œ ê° ì‹ ê²½ë§ì˜ ë³€ìˆ˜ë“¤ì„ ë”°ë¡œ í•™ìŠµì‹œì¼œì•¼ í–ˆë‹¤. í•˜ì§€ë§Œ tf.layersë¥¼ ì‚¬ìš©í•˜ë©´ ë³€ìˆ˜ë¥¼ ì„ ì–¸í•˜ì§€ ì•Šê³  tf.variable_scopeë¥¼ ì´ìš©í•´ ìŠ¤ì½”í”„ë¥¼ ì§€ì •í•´ì¤„ ìˆ˜ ìˆë‹¤.

       def generator(noise, labels):
          with tf.variable_scope('generator'):
              inputs = tf.concat([noise,labels],1)    # tf.concatí•¨ìˆ˜ë¥¼ ì´ìš©í•´ noiseê°’ì— labelsì •ë³´ë¥¼ ê°„ë‹¨í•˜ê²Œ ì¶”ê°€
              hidden = tf.layers.dense(inputs,n_hidden, activation = tf.nn.relu)
              output = tf.layers.dense(hidden,n_input, activation = tf.nn.sigmoid)
             
          return output

3. ìƒì„±ì ì‹ ê²½ë§ê³¼ ê°™ì€ ë°©ë²•ìœ¼ë¡œ êµ¬ë¶„ì ì‹ ê²½ë§ì„ ë§Œë“ ë‹¤. ì—¬ê¸°ì„œ ì£¼ì˜í•  ì ì€ ì§„ì§œ ì´ë¯¸ì§€ë¥¼ íŒë³„í•  ë•Œì™€ ê°€ì§œ ì´ë¯¸ì§€ë¥¼ íŒë³„í•  ë•Œ ë˜‘ê°™ì€ ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•´ì•¼ í•œë‹¤ëŠ” ì ì´ë‹¤. ê·¸ëŸ¬ê¸° ìœ„í•´ scope.reuse_variablesí•¨ìˆ˜ë¥¼ ì´ìš©í•´ ì´ì „ì— ì‚¬ìš©í•œ ë³€ìˆ˜ë¥¼ ì¬ì‚¬ìš©í•˜ë„ë¡ ì§ ë‹¤.

       def discriminator(inputs, labels, reuse = None):
          with tf.variable_scope('discriminaor') as scope :
              if reuse : 
                  scope.reuse_variables()
              input = tf.concat([inputs, labels],1)
              hidden = tf.layers.dense(inputs, n_hidden, activation = tf.nn.relu)
              output = tf.layers.dense(hidden, 1, activation = None)   # ì¶œë ¥ê°’ì— í™œì„±í™” í•¨ìˆ˜ ì‚¬ìš© ì•ˆ í•¨ -> ì†ì‹¤ê°’ ê³„ì‚°ì— sigmoid_cross_entropy_with_logitsí•¨ìˆ˜ ì‚¬ìš©í•˜ê¸° ìœ„í•¨
              
          return output
          
4. ë…¸ì´ì¦ˆ ìƒì„± ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ì—ì„œ ì´ë²ˆì—ëŠ” ë…¸ì´ì¦ˆë¥¼ ê· ë“±ë¶„í¬ë¡œ ìƒì„±í•˜ë„ë¡ ì‘ì„±í•œë‹¤.

       def get_noise(batch_size, n_noise):
          return np.random.uniform(-1.,1., size = [batch_size, n_noise])
          
5. ìƒì„±ìë¥¼ êµ¬í•˜ê³  ì§„ì§œ ì´ë¯¸ì§€ ë°ì´í„°ì™€ ìƒì„±ìê°€ ë§Œë“  ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ì´ìš©í•˜ëŠ” êµ¬ë¶„ìë¥¼ í•˜ë‚˜ì”© ë§Œë“¤ì–´ì¤€ë‹¤. ì´ ë•Œ ìƒì„±ìì—ëŠ” ë ˆì´ë¸” ì •ë³´ë¥¼ ì¶”ê°€í•˜ì—¬ ì¶”í›„ ë ˆì´ë¸” ì •ë³´ì— í•´ë‹¹í•˜ëŠ” ì´ë¯¸ì§€ë¥¼ ìƒì„±í•  ìˆ˜ ìˆë„ë¡ ìœ ë„í•œë‹¤. ê·¸ë¦¬ê³  ê°€ì§œ ì´ë¯¸ì§€ êµ¬ë¶„ìë¥¼ ë§Œë“¤ ë•ŒëŠ” ì§„ì§œ ì´ë¯¸ì§€ êµ¬ë¶„ìì—ì„œ ì‚¬ìš©í•œ ë³€ìˆ˜ë“¤ì„ ì¬ì‚¬ìš©í•˜ë„ë¡ reuseì˜µì…˜ì„ Trueë¡œ ì„¤ì •í•œë‹¤.

       G = generator(Z,Y)
       D_real = discriminator(X,Y)
       D_gene = discriminator(G,Y, True)
       
6. ì†ì‹¤ í•¨ìˆ˜ë¥¼ ë§Œë“¤ ì°¨ë¡€ì´ë‹¤. ì•ê³¼ ë˜‘ê°™ì´ ì§„ì§œ ì´ë¯¸ì§€ë¥¼ íŒë³„í•˜ëŠ” D_realì€ 1ì— ê°€ê¹Œì›Œì§€ë„ë¡í•˜ê³  ê°€ì§œ ì´ë¯¸ì§€ë¥¼ íŒë³„í•˜ëŠ” D_geneê°’ì€ 0ì— ê°€ê¹Œì›Œì§€ë„ë¡ í•˜ëŠ” ê²ƒì´ì§€ë§Œ simoid_cross_entropy_with_logits í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ë©´ ì½”ë“œë¥¼ ì¢€ ë” ê°„í¸í•˜ê²Œ ì‘ì„±í•  ìˆ˜ ìˆë‹¤.

       loss_D_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = D_real, labels = tf.ones_like(D_real)))  # D_real ê²°ê³¼ê°’ê³¼ D_realí¬ê¸°ë§Œí¼ 1ë¡œ ì±„ìš´ ê°’ë“¤ì„ ë¹„êµ
       loss_D_gene = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = D_gene, labels = tf.zeros_like(D_gene))  # D_gene ê²°ê³¼ê°’ê³¼ D_geneí¬ê¸°ë§Œí¼ 0ìœ¼ë¡œ ì±„ìš´ ê°’ë“¤ì„ ë¹„êµ
       loss D = loss_D_real + loss_D_gene   # ì´ ê°’ì„ ìµœì†Œí™”í•˜ë©´ êµ¬ë¶„ìë¥¼ í•™ìŠµì‹œí‚¬ ìˆ˜ ìˆìŒ

   
7. ê·¸ëŸ° ë‹¤ìŒ loss_Gë¥¼ êµ¬í•œë‹¤. loss_GëŠ” ìƒì„±ìë¥¼ í•™ìŠµì‹œí‚¤ê¸° ìœ„í•œ ì†ì‹¤ê°’ìœ¼ë¡œ sigmoid_cross_entropy_with_logitsí•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ D_geneë¥¼ 1ì— ê°€ê¹ê²Œ ë§Œë“œëŠ” ê°’ì„ ì†ì‹¤ê°’ìœ¼ë¡œ ì·¨í•˜ë„ë¡ í•œë‹¤.

       loss_G = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = D_gene, labels = tf.ones_like(D_gene)))
       
8. ë§ˆì§€ë§‰ìœ¼ë¡œ í…ì„œí”Œë¡œê°€ ì œê³µí•˜ëŠ” tf.get_collection í•¨ìˆ˜ë¥¼ ì´ìš©í•´ discriminatorì™€ generatorìŠ¤ì½”í”„ì—ì„œ ì‚¬ìš©ëœ ë³€ìˆ˜ë“¤ì„ ê°€ì ¸ì˜¨ ë’¤ ì´ ë³€ìˆ˜ë“¤ì„ ìµœì í™”ì— ì‚¬ìš©í•  ê°ê°ì˜ ì†ì‹¤ í•¨ìˆ˜ì™€ í•¨ê»˜ ìµœì í™” í•¨ìˆ˜ì— ë„£ì–´ í•™ìŠµ ëª¨ë¸ êµ¬ì„±ì„ ë§ˆë¬´ë¦¬ í•œë‹¤.

       vars_D = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope = 'discriminator')
       vars_G = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope = 'generator')
       
       train_D = tf.train.AdamOptimizer().minimize(loss_D, var_list = vars_D)
       train_G = tf.train.AdamOptimizer().minimize(loss_G, var_list = vars_G)

9. í•™ìŠµì„ ì§„í–‰í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•œë‹¤. ì•ì„œ ë§Œë“  GAN ëª¨ë¸ê³¼ ê±°ì˜ ë˜‘ê°™ì§€ë§Œ í”Œë ˆì´ìŠ¤í™€ë” Yì˜ ì…ë ¥ê°’ìœ¼ë¡œ batch_ysê°’ì„ ë„£ì–´ì¤€ë‹¤ëŠ” ê²ƒë§Œ ì£¼ì˜í•˜ë©´ ëœë‹¤.

       sess = tf.Session()
       sess.run(tf.global_variables_initializer())
       
       total_batch = int(mnist.train.num_examples/batch_size)
       loss_val_D, loss_val_G = 0,0
       
       for epoch in range(total_epoch):
          for i in range(total_batch):
              batch_xs, batch_ys = mnist.train.next_batch(batch_size)
              noise = get_noise(batch_size,n_noise)
              
              _, loss_val_D = sess.run([train_D, loss_D], feed_dict = {X:batch_xs, Y: batch_ys, Z:noise})
              _, loss_val_G = sess.run([train_G, loss_G], feed_dict = {Y:batch_ys, Z: noise})
              
          print('Epoch:', '%04d' % epoch, 'D loss: {:.4}'.format(loss_val_D), 'G loss: {:.4}'.format(loss_val_G))
       

10. í•™ìŠµ ì¤‘ê°„ì¤‘ê°„ì— ìƒì„±ìë¡œ ë§Œë“  ì´ë¯¸ì§€ë¥¼ ì €ì¥í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•œë‹¤. í”Œë ˆì´ìŠ¤í™€ë” Yì˜ ì…ë ¥ê°’ì„ ë„£ì–´ì¤€ë‹¤ëŠ” ê²ƒì´ ë‹¤ë¥´ê³  ì§„ì§œ ì´ë¯¸ì§€ì™€ ë¹„êµí•´ë³´ê¸° ìœ„í•´ ìœ„ìª½ì—ëŠ” ì§„ì§œ ì´ë¯¸ì§€ë¥¼ ì¶œë ¥í•˜ê³  ì•„ë˜ìª½ì—ëŠ” ìƒì„±í•œ ì´ë¯¸ì§€ë¥¼ ì¶œë ¥í•˜ë„ë¡ í•˜ì˜€ë‹¤.

        if epoch == 0 or (epoch+1) % 10 == 0:
            sample_size = 10
            noise = get_noise(sample_size, n_noise)
            samples = sess.run(G, feed_dict = {Y: mnist.test.labels[:sample_size], Z:noise})
            
            fit, ax = plt.subplots(2,sample_size, figsize=(sample_size,2))
            
            for i in range(sample_size):
                ax[0][i].set_axis_off()
                ax[1][i].set_axis_off()
                
                ax[0][i].imshow(np.reshape(mnist.test.images[i], (28,28)))
                ax[1][i].imshow(np.reshape(samples[i], (28,28)))
                
            plt.savefig('samples2/{}.png'.format(str(epoch).zfill(3)), bbox_inches='tight')
            plt.close(fig)
         
        print('ìµœì í™” ì™„ë£Œ!')



















          
          
          
          
          
