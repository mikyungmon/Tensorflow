{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28df5795",
   "metadata": {},
   "source": [
    "# Chapter 09"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c313a0",
   "metadata": {},
   "source": [
    "1️⃣ GAN 기본 모델 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3149fe70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/data/train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0000 D loss: -0.6594 G loss: -2.023\n",
      "Epoch: 0001 D loss: -0.3097 G loss: -2.441\n",
      "Epoch: 0002 D loss: -0.115 G loss: -3.244\n",
      "Epoch: 0003 D loss: -0.7724 G loss: -1.324\n",
      "Epoch: 0004 D loss: -0.2095 G loss: -2.277\n",
      "Epoch: 0005 D loss: -0.2602 G loss: -2.342\n",
      "Epoch: 0006 D loss: -0.137 G loss: -3.055\n",
      "Epoch: 0007 D loss: -0.08937 G loss: -3.77\n",
      "Epoch: 0008 D loss: -0.1996 G loss: -2.762\n",
      "Epoch: 0009 D loss: -0.3722 G loss: -2.281\n",
      "Epoch: 0010 D loss: -0.2851 G loss: -2.64\n",
      "Epoch: 0011 D loss: -0.2772 G loss: -2.688\n",
      "Epoch: 0012 D loss: -0.3727 G loss: -2.561\n",
      "Epoch: 0013 D loss: -0.2267 G loss: -3.018\n",
      "Epoch: 0014 D loss: -0.3645 G loss: -2.723\n",
      "Epoch: 0015 D loss: -0.3474 G loss: -2.752\n",
      "Epoch: 0016 D loss: -0.3949 G loss: -2.646\n",
      "Epoch: 0017 D loss: -0.3025 G loss: -2.75\n",
      "Epoch: 0018 D loss: -0.4109 G loss: -2.708\n",
      "Epoch: 0019 D loss: -0.2817 G loss: -2.993\n",
      "Epoch: 0020 D loss: -0.4049 G loss: -2.834\n",
      "Epoch: 0021 D loss: -0.3441 G loss: -2.774\n",
      "Epoch: 0022 D loss: -0.391 G loss: -2.68\n",
      "Epoch: 0023 D loss: -0.3999 G loss: -2.797\n",
      "Epoch: 0024 D loss: -0.4017 G loss: -3.079\n",
      "Epoch: 0025 D loss: -0.1654 G loss: -3.061\n",
      "Epoch: 0026 D loss: -0.3017 G loss: -3.242\n",
      "Epoch: 0027 D loss: -0.4919 G loss: -2.775\n",
      "Epoch: 0028 D loss: -0.4709 G loss: -2.586\n",
      "Epoch: 0029 D loss: -0.4909 G loss: -2.426\n",
      "Epoch: 0030 D loss: -0.3806 G loss: -2.86\n",
      "Epoch: 0031 D loss: -0.5048 G loss: -2.743\n",
      "Epoch: 0032 D loss: -0.4481 G loss: -3.048\n",
      "Epoch: 0033 D loss: -0.3623 G loss: -3.102\n",
      "Epoch: 0034 D loss: -0.3696 G loss: -2.952\n",
      "Epoch: 0035 D loss: -0.4871 G loss: -3.03\n",
      "Epoch: 0036 D loss: -0.6205 G loss: -2.58\n",
      "Epoch: 0037 D loss: -0.4322 G loss: -2.503\n",
      "Epoch: 0038 D loss: -0.4424 G loss: -2.591\n",
      "Epoch: 0039 D loss: -0.522 G loss: -2.536\n",
      "Epoch: 0040 D loss: -0.5395 G loss: -2.279\n",
      "Epoch: 0041 D loss: -0.5774 G loss: -2.396\n",
      "Epoch: 0042 D loss: -0.5867 G loss: -2.642\n",
      "Epoch: 0043 D loss: -0.4128 G loss: -2.541\n",
      "Epoch: 0044 D loss: -0.3893 G loss: -2.491\n",
      "Epoch: 0045 D loss: -0.6271 G loss: -2.886\n",
      "Epoch: 0046 D loss: -0.5165 G loss: -2.494\n",
      "Epoch: 0047 D loss: -0.6405 G loss: -2.464\n",
      "Epoch: 0048 D loss: -0.6183 G loss: -2.581\n",
      "Epoch: 0049 D loss: -0.6487 G loss: -2.391\n",
      "Epoch: 0050 D loss: -0.5869 G loss: -2.267\n",
      "Epoch: 0051 D loss: -0.6255 G loss: -2.215\n",
      "Epoch: 0052 D loss: -0.5624 G loss: -2.278\n",
      "Epoch: 0053 D loss: -0.6357 G loss: -2.007\n",
      "Epoch: 0054 D loss: -0.5855 G loss: -2.423\n",
      "Epoch: 0055 D loss: -0.4302 G loss: -2.537\n",
      "Epoch: 0056 D loss: -0.6973 G loss: -2.382\n",
      "Epoch: 0057 D loss: -0.6321 G loss: -2.063\n",
      "Epoch: 0058 D loss: -0.651 G loss: -2.141\n",
      "Epoch: 0059 D loss: -0.7869 G loss: -1.865\n",
      "Epoch: 0060 D loss: -0.555 G loss: -2.093\n",
      "Epoch: 0061 D loss: -0.6551 G loss: -2.254\n",
      "Epoch: 0062 D loss: -0.6402 G loss: -2.21\n",
      "Epoch: 0063 D loss: -0.5879 G loss: -2.278\n",
      "Epoch: 0064 D loss: -0.6306 G loss: -1.879\n",
      "Epoch: 0065 D loss: -0.5412 G loss: -2.174\n",
      "Epoch: 0066 D loss: -0.5704 G loss: -2.192\n",
      "Epoch: 0067 D loss: -0.6934 G loss: -2.2\n",
      "Epoch: 0068 D loss: -0.623 G loss: -2.259\n",
      "Epoch: 0069 D loss: -0.6809 G loss: -2.173\n",
      "Epoch: 0070 D loss: -0.6158 G loss: -2.228\n",
      "Epoch: 0071 D loss: -0.5706 G loss: -2.144\n",
      "Epoch: 0072 D loss: -0.6685 G loss: -1.956\n",
      "Epoch: 0073 D loss: -0.7233 G loss: -1.873\n",
      "Epoch: 0074 D loss: -0.6127 G loss: -2.266\n",
      "Epoch: 0075 D loss: -0.6532 G loss: -1.952\n",
      "Epoch: 0076 D loss: -0.6808 G loss: -2.032\n",
      "Epoch: 0077 D loss: -0.5491 G loss: -2.286\n",
      "Epoch: 0078 D loss: -0.6363 G loss: -2.074\n",
      "Epoch: 0079 D loss: -0.7377 G loss: -2.275\n",
      "Epoch: 0080 D loss: -0.6442 G loss: -2.194\n",
      "Epoch: 0081 D loss: -0.8052 G loss: -1.847\n",
      "Epoch: 0082 D loss: -0.56 G loss: -2.286\n",
      "Epoch: 0083 D loss: -0.7071 G loss: -1.873\n",
      "Epoch: 0084 D loss: -0.5817 G loss: -2.266\n",
      "Epoch: 0085 D loss: -0.5836 G loss: -2.025\n",
      "Epoch: 0086 D loss: -0.6913 G loss: -1.691\n",
      "Epoch: 0087 D loss: -0.9453 G loss: -1.704\n",
      "Epoch: 0088 D loss: -0.7094 G loss: -1.982\n",
      "Epoch: 0089 D loss: -0.6357 G loss: -1.929\n",
      "Epoch: 0090 D loss: -0.7995 G loss: -2.115\n",
      "Epoch: 0091 D loss: -0.7785 G loss: -1.904\n",
      "Epoch: 0092 D loss: -0.673 G loss: -1.987\n",
      "Epoch: 0093 D loss: -0.7556 G loss: -2.046\n",
      "Epoch: 0094 D loss: -0.6563 G loss: -1.941\n",
      "Epoch: 0095 D loss: -0.6415 G loss: -1.689\n",
      "Epoch: 0096 D loss: -0.6815 G loss: -1.79\n",
      "Epoch: 0097 D loss: -0.7878 G loss: -2.107\n",
      "Epoch: 0098 D loss: -0.6918 G loss: -2.122\n",
      "Epoch: 0099 D loss: -0.8106 G loss: -2.0\n",
      "최적화 완료!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"./mnist/data/\", one_hot = True)\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "total_epoch = 100\n",
    "batch_size = 100\n",
    "learning_rate = 0.0002\n",
    "n_hidden = 256\n",
    "n_input = 28 * 28\n",
    "n_noise = 128\n",
    "\n",
    "### 신경망 모델 구성  ###\n",
    "\n",
    "# 비지도 학습이므로 Y를 사용하지 않음\n",
    "X = tf.placeholder(tf.float32, [None,n_input])\n",
    "Z = tf.placeholder(tf.float32, [None,n_noise])   # 노이즈를 입력할 플레이스홀더 Z\n",
    "\n",
    "# 생성자 신경망에서 사용할 변수 설정\n",
    "G_W1 = tf.Variable(tf.random_normal([n_noise,n_hidden], stddev = 0.01))  # 첫 번째 가중치, 편향 -> 은닉층으로 출력하기 위한 변수들\n",
    "G_b1 = tf.Variable(tf.zeros([n_hidden]))\n",
    "G_W2 = tf.Variable(tf.random_normal([n_hidden, n_input], stddev = 0.01))  # 두 번째 가중치, 편향 -> 출력층에 사용할 변수들\n",
    "G_b2 = tf.Variable(tf.zeros(n_input))\n",
    "\n",
    "# 구분자 신경망에서 사용할 변수 설정\n",
    "D_W1 = tf.Variable(tf.random_normal([n_input, n_hidden], stddev = 0.01))\n",
    "D_b1 = tf.Variable(tf.zeros([n_hidden]))\n",
    "D_W2 = tf.Variable(tf.random_normal([n_hidden, 1], stddev = 0.01))\n",
    "D_b2 = tf.Variable(tf.zeros([1]))\n",
    "\n",
    "# 생성자 신경망 구현\n",
    "# 생성자는 무작위로 생성한 노이즈를 받아 가중치와 편향을 반영하여 은닉층을 만들고 은닉층에서 실제 이미지와 같은 크기의 결과를 출력하는 구성\n",
    "def generator(noise_z):\n",
    "    hidden = tf.nn.relu(tf.matmul(noise_z,G_W1) + G_b1)  \n",
    "    output = tf.nn.sigmoid(tf.matmul(hidden,G_W2) + G_b2)\n",
    "    \n",
    "    return output\n",
    "\n",
    "# 구분자 신경망 구현\n",
    "def discriminator(inputs):\n",
    "    hidden = tf.nn.relu(tf.matmul(inputs, D_W1) + D_b1)\n",
    "    output = tf.nn.sigmoid(tf.matmul(hidden, D_W2) + D_b2)\n",
    "    \n",
    "    return output\n",
    "\n",
    "# 무작위한 노이즈 만들어 주는 함수 \n",
    "def get_noise(batch_size, n_noise):\n",
    "    return np.random.normal(size = (batch_size, n_noise))\n",
    "\n",
    "G = generator(Z)   # 노이즈 Z 이용해 생성자로 가짜 이미지 생성\n",
    "D_gene = discriminator(G)   # G가 생성한 가짜 이미지를 구분자에 넣어 판별\n",
    "D_real = discriminator(X)   # 진짜 이미지를 구분자에 넣어 판별\n",
    "\n",
    "loss_D = tf.reduce_mean(tf.log(D_real) + tf.log(1-D_gene))   # D_real -> 1에 가까워야 함 / D_gene -> 0에 가까워야 함\n",
    "loss_G = tf.reduce_mean(tf.log(D_gene))\n",
    "\n",
    "# 생성자와 구별자 각각 loss구할 때 해당 신경망에 사용되는 변수들만 최적화해야 함\n",
    "D_val_list = [D_W1,D_b1,D_W2,D_b2]\n",
    "G_val_list = [G_W1,G_b1,G_W2,G_b2]\n",
    "\n",
    "train_D = tf.train.AdamOptimizer(learning_rate).minimize(-loss_D, var_list = D_val_list)\n",
    "train_G = tf.train.AdamOptimizer(learning_rate).minimize(-loss_G, var_list = G_val_list)\n",
    "\n",
    "### 신경망 모델 학습  ###\n",
    "\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "total_batch = int(mnist.train.num_examples / batch_size)\n",
    "loss_val_D , loss_val_G = 0, 0\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        noise = get_noise(batch_size, n_noise)\n",
    "        \n",
    "        _, loss_val_D = sess.run([train_D, loss_D], feed_dict = {X:batch_xs, Z:noise})\n",
    "        _, loss_val_G = sess.run([train_G, loss_G], feed_dict = {Z:noise})\n",
    "        \n",
    "    print('Epoch:', '%04d' % epoch, 'D loss: {:.4}'.format(loss_val_D), 'G loss: {:.4}'.format(loss_val_G))\n",
    "    \n",
    "    # 확인용 이미지 생성\n",
    "    if epoch == 0 or (epoch+1) % 10 ==0:  # 학습이 잘 되는지는 0,9,19,29번째,, ~ 마다 생성기로 이미지를 생성하여 눈으로 직접 확인\n",
    "        sample_size = 10\n",
    "        noise = get_noise(sample_size, n_noise)\n",
    "        samples = sess.run(G, feed_dict = {Z:noise})   # 노이즈를 만들고 G에 넣어 결과값 만듦\n",
    "        \n",
    "        # 결과값들을 28 * 28 크기의 가짜 이미지로 만들어 samples폴더에 저장하도록함\n",
    "        fig, ax = plt.subplots(1, sample_size, figsize = (sample_size,1))\n",
    "        \n",
    "        for i in range(sample_size):\n",
    "            ax[i].set_axis_off()\n",
    "            ax[i].imshow(np.reshape(samples[i],(28,28)))\n",
    "            \n",
    "        plt.savefig('samples/{}.png'.format(str(epoch).zfill(3)),bbox_inches ='tight')\n",
    "        plt.close(fig)\n",
    "        \n",
    "print('최적화 완료!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d685c9f",
   "metadata": {},
   "source": [
    "2️⃣ 원하는 숫자 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8bc58f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/data/train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0000 D loss: 0.008796 G loss: 7.532\n",
      "Epoch: 0001 D loss: 0.01799 G loss: 7.157\n",
      "Epoch: 0002 D loss: 0.01457 G loss: 7.024\n",
      "Epoch: 0003 D loss: 0.005089 G loss: 7.678\n",
      "Epoch: 0004 D loss: 0.03772 G loss: 7.238\n",
      "Epoch: 0005 D loss: 0.1079 G loss: 6.165\n",
      "Epoch: 0006 D loss: 0.0443 G loss: 7.801\n",
      "Epoch: 0007 D loss: 0.0501 G loss: 7.76\n",
      "Epoch: 0008 D loss: 0.08447 G loss: 9.119\n",
      "Epoch: 0009 D loss: 0.08855 G loss: 6.449\n",
      "Epoch: 0010 D loss: 0.158 G loss: 5.661\n",
      "Epoch: 0011 D loss: 0.4178 G loss: 4.368\n",
      "Epoch: 0012 D loss: 0.4011 G loss: 3.826\n",
      "Epoch: 0013 D loss: 0.2167 G loss: 5.695\n",
      "Epoch: 0014 D loss: 0.3046 G loss: 4.714\n",
      "Epoch: 0015 D loss: 0.5512 G loss: 3.296\n",
      "Epoch: 0016 D loss: 0.5108 G loss: 3.772\n",
      "Epoch: 0017 D loss: 0.5404 G loss: 3.464\n",
      "Epoch: 0018 D loss: 0.5879 G loss: 2.976\n",
      "Epoch: 0019 D loss: 0.7401 G loss: 3.024\n",
      "Epoch: 0020 D loss: 0.7218 G loss: 3.063\n",
      "Epoch: 0021 D loss: 0.7091 G loss: 2.751\n",
      "Epoch: 0022 D loss: 0.7257 G loss: 2.708\n",
      "Epoch: 0023 D loss: 0.5764 G loss: 3.198\n",
      "Epoch: 0024 D loss: 0.8273 G loss: 2.629\n",
      "Epoch: 0025 D loss: 0.5614 G loss: 2.765\n",
      "Epoch: 0026 D loss: 0.9663 G loss: 2.458\n",
      "Epoch: 0027 D loss: 0.7007 G loss: 2.539\n",
      "Epoch: 0028 D loss: 0.6468 G loss: 2.226\n",
      "Epoch: 0029 D loss: 0.6294 G loss: 2.457\n",
      "Epoch: 0030 D loss: 0.8831 G loss: 1.875\n",
      "Epoch: 0031 D loss: 0.6166 G loss: 2.342\n",
      "Epoch: 0032 D loss: 0.7942 G loss: 2.257\n",
      "Epoch: 0033 D loss: 0.7446 G loss: 2.091\n",
      "Epoch: 0034 D loss: 0.8276 G loss: 2.092\n",
      "Epoch: 0035 D loss: 0.8476 G loss: 2.172\n",
      "Epoch: 0036 D loss: 0.9164 G loss: 1.764\n",
      "Epoch: 0037 D loss: 0.8017 G loss: 2.043\n",
      "Epoch: 0038 D loss: 0.6486 G loss: 2.2\n",
      "Epoch: 0039 D loss: 0.7272 G loss: 2.219\n",
      "Epoch: 0040 D loss: 0.7512 G loss: 2.026\n",
      "Epoch: 0041 D loss: 0.6732 G loss: 2.215\n",
      "Epoch: 0042 D loss: 0.6918 G loss: 1.924\n",
      "Epoch: 0043 D loss: 0.6279 G loss: 2.128\n",
      "Epoch: 0044 D loss: 0.8368 G loss: 1.91\n",
      "Epoch: 0045 D loss: 0.7429 G loss: 2.055\n",
      "Epoch: 0046 D loss: 0.6836 G loss: 2.119\n",
      "Epoch: 0047 D loss: 0.861 G loss: 2.123\n",
      "Epoch: 0048 D loss: 0.6399 G loss: 2.366\n",
      "Epoch: 0049 D loss: 0.6461 G loss: 2.781\n",
      "Epoch: 0050 D loss: 0.9165 G loss: 2.012\n",
      "Epoch: 0051 D loss: 0.8434 G loss: 1.824\n",
      "Epoch: 0052 D loss: 1.002 G loss: 1.728\n",
      "Epoch: 0053 D loss: 0.8201 G loss: 2.175\n",
      "Epoch: 0054 D loss: 0.6246 G loss: 2.107\n",
      "Epoch: 0055 D loss: 0.8746 G loss: 2.029\n",
      "Epoch: 0056 D loss: 0.8413 G loss: 2.149\n",
      "Epoch: 0057 D loss: 0.8149 G loss: 2.278\n",
      "Epoch: 0058 D loss: 0.7303 G loss: 2.05\n",
      "Epoch: 0059 D loss: 1.031 G loss: 2.09\n",
      "Epoch: 0060 D loss: 0.7623 G loss: 1.987\n",
      "Epoch: 0061 D loss: 0.7818 G loss: 2.013\n",
      "Epoch: 0062 D loss: 0.6489 G loss: 2.162\n",
      "Epoch: 0063 D loss: 0.7973 G loss: 1.856\n",
      "Epoch: 0064 D loss: 0.8147 G loss: 2.411\n",
      "Epoch: 0065 D loss: 0.7511 G loss: 1.937\n",
      "Epoch: 0066 D loss: 0.9472 G loss: 2.053\n",
      "Epoch: 0067 D loss: 0.8208 G loss: 1.943\n",
      "Epoch: 0068 D loss: 0.7966 G loss: 1.874\n",
      "Epoch: 0069 D loss: 0.7485 G loss: 2.075\n",
      "Epoch: 0070 D loss: 0.7393 G loss: 2.224\n",
      "Epoch: 0071 D loss: 0.8228 G loss: 1.927\n",
      "Epoch: 0072 D loss: 0.8069 G loss: 1.994\n",
      "Epoch: 0073 D loss: 0.7063 G loss: 2.304\n",
      "Epoch: 0074 D loss: 0.7468 G loss: 2.523\n",
      "Epoch: 0075 D loss: 0.7666 G loss: 1.781\n",
      "Epoch: 0076 D loss: 0.6125 G loss: 2.198\n",
      "Epoch: 0077 D loss: 0.7367 G loss: 2.055\n",
      "Epoch: 0078 D loss: 0.7682 G loss: 2.161\n",
      "Epoch: 0079 D loss: 0.6256 G loss: 1.922\n",
      "Epoch: 0080 D loss: 0.948 G loss: 2.074\n",
      "Epoch: 0081 D loss: 0.9328 G loss: 2.059\n",
      "Epoch: 0082 D loss: 0.7899 G loss: 2.161\n",
      "Epoch: 0083 D loss: 0.6643 G loss: 1.999\n",
      "Epoch: 0084 D loss: 0.9229 G loss: 1.875\n",
      "Epoch: 0085 D loss: 0.7166 G loss: 2.17\n",
      "Epoch: 0086 D loss: 0.818 G loss: 1.89\n",
      "Epoch: 0087 D loss: 0.8683 G loss: 1.734\n",
      "Epoch: 0088 D loss: 0.7584 G loss: 1.843\n",
      "Epoch: 0089 D loss: 0.6252 G loss: 2.113\n",
      "Epoch: 0090 D loss: 0.7538 G loss: 2.088\n",
      "Epoch: 0091 D loss: 0.8088 G loss: 1.879\n",
      "Epoch: 0092 D loss: 0.7645 G loss: 2.088\n",
      "Epoch: 0093 D loss: 0.651 G loss: 2.009\n",
      "Epoch: 0094 D loss: 0.6425 G loss: 2.012\n",
      "Epoch: 0095 D loss: 0.7326 G loss: 1.99\n",
      "Epoch: 0096 D loss: 0.66 G loss: 2.21\n",
      "Epoch: 0097 D loss: 0.7556 G loss: 1.947\n",
      "Epoch: 0098 D loss: 0.6588 G loss: 1.995\n",
      "Epoch: 0099 D loss: 0.7335 G loss: 1.846\n",
      "최적화 완료!\n"
     ]
    }
   ],
   "source": [
    "# 노이즈에 레이블 데이터를 힌트로 넣어주는 방법을 사용\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"./mnist/data/\", one_hot = True)\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "total_epoch = 100\n",
    "batch_size = 100\n",
    "n_hidden = 256\n",
    "n_input = 28 * 28\n",
    "n_noise = 128\n",
    "n_class = 10\n",
    "\n",
    "### 신경망 모델 구성  ###\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None,n_input])\n",
    "Y = tf.placeholder(tf.float32, [None,n_class])   # 결과값 판정용 아니고 노이즈와 실제 이미지에 해당하는 숫자를 힌트로 넣어주는 용도\n",
    "Z = tf.placeholder(tf.float32, [None,n_noise])   # 노이즈를 입력할 플레이스홀더 Z\n",
    "\n",
    "# 생성자 신경망 구현\n",
    "# 변수들을 선언하지 않고 tf.layers사용 -> 변수 선언하지 않고 tf.variable_scope이용해 스코프 지정 가능\n",
    "def generator_2(noise,labels):\n",
    "    with tf.variable_scope('generator_2'):\n",
    "        inputs = tf.concat([noise,labels],1)  # tf.concat함수를 이용해 noise값에 labels정보를 간단하게 추가\n",
    "        hidden = tf.layers.dense(inputs, n_hidden, activation=tf.nn.relu)\n",
    "        output = tf.layers.dense(hidden, n_input, activation=tf.nn.sigmoid)\n",
    "    \n",
    "    return output\n",
    "\n",
    "# 구분자 신경망 구현\n",
    "# 주의! 진짜 이미지를 판별할 때와 가짜 이미지를 판별할 때 똑같은 변수를 사용해야 함. 그러기 위해 scope.reuse_variables함수를 이용해 이전에 사용한 변수를 재사용하도록 구현.\n",
    "def discriminator_2(inputs,labels, reuse = None):\n",
    "    with tf.variable_scope('discriminator_2') as scope:\n",
    "        if reuse :\n",
    "            scope.reuse_variables()\n",
    "            \n",
    "        inputs = tf.concat([inputs,labels],1)\n",
    "        hidden = tf.layers.dense(inputs, n_hidden, activation=tf.nn.relu)\n",
    "        output = tf.layers.dense(hidden,1, activation=None)\n",
    "\n",
    "    return output\n",
    "\n",
    "# 무작위한 노이즈 만들어 주는 함수 -> 노이즈를 균등분포로 생성\n",
    "def get_noise(batch_size, n_noise):\n",
    "    return np.random.uniform(-1.,1.,size = [batch_size, n_noise])\n",
    "\n",
    "G = generator_2(Z,Y)   # 노이즈 Z 이용해 생성자로 가짜 이미지 생성. 레이블 정보 추가하여 레이블 정보에 해당하는 이미지 생성하도록 유도\n",
    "D_gene = discriminator_2(G,Y)   # G가 생성한 가짜 이미지를 구분자에 넣어 판별\n",
    "D_real = discriminator_2(X,Y,True)   # 진짜 이미지를 구분자에 넣어 판별. 사용한 변수들을 재사용하도록 reuse옵션을 true로!\n",
    "\n",
    "loss_D_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = D_real, labels = tf.ones_like(D_real)))\n",
    "loss_D_gene = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = D_gene, labels = tf.zeros_like(D_gene)))\n",
    "loss_D = loss_D_real + loss_D_gene  # 이 값을 최소화하면 구분자를 학습시킬 수 있음\n",
    "\n",
    "loss_G = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = D_gene, labels = tf.ones_like(D_gene)))\n",
    "    \n",
    "# tf.get_collection 함수 이용해 구별자와 생성자 scope에서 사용된 변수들을 가져온 뒤 이 변수들을 최적화에 사용할 손실함수와 최적화 함수에 넣음\n",
    "vars_D = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope = 'discriminator_2')\n",
    "vars_G = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope = 'generator_2')\n",
    "train_D = tf.train.AdamOptimizer().minimize(loss_D, var_list = vars_D)\n",
    "train_G = tf.train.AdamOptimizer().minimize(loss_G, var_list = vars_G)\n",
    "\n",
    "### 신경망 모델 학습  ###\n",
    "\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "total_batch = int(mnist.train.num_examples / batch_size)\n",
    "loss_val_D , loss_val_G = 0, 0\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        noise = get_noise(batch_size, n_noise)\n",
    "        \n",
    "        _, loss_val_D = sess.run([train_D, loss_D], feed_dict = {X:batch_xs, Y:batch_ys,Z:noise})\n",
    "        _, loss_val_G = sess.run([train_G, loss_G], feed_dict = {Y:batch_ys, Z:noise})\n",
    "        \n",
    "    print('Epoch:', '%04d' % epoch, 'D loss: {:.4}'.format(loss_val_D), 'G loss: {:.4}'.format(loss_val_G))\n",
    "    \n",
    "    # 확인용 이미지 생성\n",
    "    if epoch == 0 or (epoch+1) % 10 ==0:  # 학습이 잘 되는지는 0,9,19,29번째,, ~ 마다 생성기로 이미지를 생성하여 눈으로 직접 확인\n",
    "        sample_size = 10\n",
    "        noise = get_noise(sample_size, n_noise)\n",
    "        # 플레이스홀더 Y의 입력값을 넣어준다는 것이 위 코드와 다른점\n",
    "        samples = sess.run(G, feed_dict = {Y:mnist.test.labels[:sample_size], Z:noise})   # 노이즈를 만들고 G에 넣어 결과값 만듦\n",
    "        \n",
    "        # 결과값들을 28 * 28 크기의 가짜 이미지로 만들어 samples폴더에 저장하도록함\n",
    "        fig, ax = plt.subplots(2, sample_size, figsize = (sample_size,2))\n",
    "        \n",
    "        for i in range(sample_size):\n",
    "            ax[0][i].set_axis_off()\n",
    "            ax[1][i].set_axis_off()\n",
    "            \n",
    "            ax[0][i].imshow(np.reshape(mnist.test.images[i],(28,28)))\n",
    "            ax[0][i].imshow(np.reshape(samples[i],(28,28)))\n",
    "            \n",
    "        plt.savefig('samples2/{}.png'.format(str(epoch).zfill(3)),bbox_inches ='tight')\n",
    "        plt.close(fig)\n",
    "        \n",
    "print('최적화 완료!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
