# Chapter 04

ì´ë²ˆ ì¥ì—ì„œëŠ” ì‹¬ì¸µ ì‹ ê²½ë§, ì¦‰ ë‹¤ì¸µ ì‹ ê²½ë§ì„ ê°„ë‹¨í•˜ê²Œ êµ¬í˜„í•´ë³¸ë‹¤.

## 4.1 ì¸ê³µì‹ ê²½ë§ì˜ ì‘ë™ ì›ë¦¬

**ì¸ê³µì‹ ê²½ë§**ì˜ ê°œë…ì€ ë‡Œë¥¼ êµ¬ì„±í•˜ëŠ” ì‹ ê²½ ì„¸í¬, ì¦‰ ë‰´ëŸ°ì˜ ë™ì‘ ì›ë¦¬ì— ê¸°ì´ˆí•œë‹¤. 

**ë‰´ëŸ°**ì˜ ê¸°ë³¸ ì›ë¦¬ëŠ” ë§¤ìš° ê°„ë‹¨í•˜ë‹¤.

ê°€ì§€ëŒê¸°ì—ì„œ ì‹ í˜¸ë¥¼ ë°›ì•„ë“¤ì´ê³ , ì´ ì‹ í˜¸ê°€ ì¶•ì‚­ëŒê¸°ë¥¼ ì§€ë‚˜ ì¶•ì‚´ë§ë‹¨ìœ¼ë¡œ ì „ë‹¬ë˜ëŠ” ê²ƒì´ë‹¤. 

ê·¸ëŸ°ë° ì¶•ì‚­ëŒê¸°ë¥¼ ì§€ë‚˜ëŠ” ë™ì•ˆ ì‹ í˜¸ê°€ ì•½í•´ì§€ê±°ë‚˜ ë„ˆë¬´ ì•½í•´ì„œ ì¶•ì‚­ë§ë‹¨ê¹Œì§€ ì „ë‹¬ë˜ì§€ ì•Šê±°ë‚˜ ë˜ëŠ” ê°•í•˜ê²Œ ì „ë‹¬ë˜ê¸°ë„ í•œë‹¤.

ê·¸ë¦¬ê³  ì¶•ì‚­ë§ë‹¨ê¹Œì§€ ì „ë‹¬ëœ ì‹ í˜¸ëŠ” ì—°ê²°ëœ ë‹¤ìŒ ë‰´ëŸ°ì˜ ê°€ì§€ëŒê¸°ë¡œ ì „ë‹¬ëœë‹¤.

ì´ëŸ¬í•œ ë‰´ëŸ°ê³¼ ì‹ ê²½ë§ì˜ ì›ë¦¬ì— ì¸ê³µ ë‰´ëŸ°ì˜ ê°œë…ì˜ ë§ì”Œìš°ë©´ ë‹¤ìŒ ê·¸ë¦¼ê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.

![image](https://user-images.githubusercontent.com/66320010/125259543-5e9eb600-e33a-11eb-849f-55af01a4c089.png)

ê·¸ë¦¼ê³¼ ê°™ì´ ì…ë ¥ ì‹ í˜¸, ì¦‰ ì…ë ¥ê°’ Xì— ê°€ì¤‘ì¹˜(W)ë¥¼ ê³±í•˜ê³  í¸í–¥(b)ì„ ë”í•œ ë’¤ í™œì„±í™” í•¨ìˆ˜(Sigmoid, ReLU ë“±)ë¥¼ ê±°ì³ ê²°ê³¼ê°’ yë¥¼ ë§Œë“¤ì–´ë‚´ëŠ” ê²ƒì´ ë°”ë¡œ ì¸ê³µ ë‰´ëŸ°ì˜ ê¸°ë³¸ì´ë‹¤.

ì›í•˜ëŠ” yê°’ì„ ë§Œë“¤ì–´ë‚´ê¸° ìœ„í•´ Wì™€ bì˜ ê°’ì„ ë³€ê²½í•´ê°€ë©´ì„œ ì ì ˆí•œ ê°’ì„ ì°¾ì•„ë‚´ëŠ” ìµœì í™” ê³¼ì •ì„ **í•™ìŠµ** ë˜ëŠ” **í›ˆë ¨**ì´ë¼ê³  í•œë‹¤.

**í™œì„±í™” í•¨ìˆ˜**ëŠ” ì¸ê³µì‹ ê²½ë§ì„ í†µê³¼í•´ì˜¨ ê°’ì„ ìµœì¢…ì ìœ¼ë¡œ ì–´ë–¤ ê°’ìœ¼ë¡œ ë§Œë“¤ì§€ ê²°ì •í•œë‹¤. ì¦‰, ì´ í•¨ìˆ˜ê°€ ì¸ê³µ ë‰´ëŸ°ì˜ í•µì‹¬ì¤‘ì—ì„œë„ ê°€ì¥ ì¤‘ìš”í•œ ìš”ì†Œì´ë‹¤.

í™œì„±í™” í•¨ìˆ˜ì—ëŠ” ëŒ€í‘œì ìœ¼ë¡œ Sigmoid, ReLU, tanhí•¨ìˆ˜ê°€ ìˆìœ¼ë©° ë‹¤ìŒ ê·¸ë¦¼ê³¼ ê°™ì€ ëª¨ì–‘ì´ë‹¤.

![image](https://user-images.githubusercontent.com/66320010/125276732-84808680-e34b-11eb-87ce-ec6a28a46a36.png)
                                
ìµœê·¼ í™œì„±í™” í•¨ìˆ˜ë¡œ ReLUí•¨ìˆ˜ë¥¼ ë§ì´ ì‚¬ìš©í•˜ëŠ”ë° ReLUëŠ” ì…ë ¥ê°’ì´ 0ë³´ë‹¤ ì‘ìœ¼ë©´ í•­ìƒ 0ì„, 0ë³´ë‹¤ í¬ë©´ ì…ë ¥ê°’ì„ ê·¸ëŒ€ë¡œ ì¶œë ¥í•œë‹¤.

âœ” ë‹¤ì‹œ ì •ë¦¬í•˜ìë©´, ì¸ê³µ ë‰´ëŸ°ì€ ê°€ì¤‘ì¹˜ì™€ í™œì„±í™” í•¨ìˆ˜ì˜ ì—°ê²°ë¡œ ì´ë£¨ì–´ì§„ ë§¤ìš° ê°„ë‹¨í•œ êµ¬ì¡°ì´ë‹¤. ì´ë ‡ê²Œ ê°„ë‹¨í•œ ê°œë…ì˜ ì¸ê³µ ë‰´ëŸ°ì„ ì¶©ë¶„íˆ ë§ì´ ì—°ê²°í•´ë†“ëŠ” ê²ƒë§Œìœ¼ë¡œ ì¸ê°„ì´ ì¸ì§€í•˜ê¸° ì–´ë ¤ìš´ ë§¤ìš° ë³µì¡í•œ íŒ¨í„´ê¹Œì§€ë„ ìŠ¤ìŠ¤ë¡œ í•™ìŠµí•  ìˆ˜ ìˆê²Œ ëœë‹¤ëŠ” ì‚¬ì‹¤ì´ ë§¤ìš° ë†€ëë‹¤.

ê·¸ëŸ¬ë‚˜ ìˆ˜ì²œ~ìˆ˜ë§Œ ê°œì˜ Wì™€ bê°’ì˜ ì¡°í•©ì„ ì¼ì¼ì´ ë³€ê²½í•´ê°€ë©° ê³„ì‚°í•˜ë ¤ë©´ ë§¤ìš° ì˜¤ëœì‹œê°„ì´ ê±¸ë¦¬ê¸° ë•Œë¬¸ì— ì‹ ê²½ë§ì„ ì œëŒ€ë¡œ í›ˆë ¨ì‹œí‚¤ê¸°ê°€ ì–´ë ¤ì› ë‹¤. 

íŠ¹íˆ ì‹ ê²½ë§ì˜ ì¸µì´ ê¹Šì–´ì§ˆìˆ˜ë¡ ì‹œë„í•´ë´ì•¼ í•˜ëŠ” ì¡°í•©ì˜ ê²½ìš°ê°€ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ëŠ˜ì–´ë‚˜ê¸° ë•Œë¬¸ì— ê³¼ê±°ì—ëŠ” ìœ ì˜ë¯¸í•œ ì‹ ê²½ë§ì„ ë§Œë“œëŠ” ê²ƒì€ ê±°ì˜ ë¶ˆê°€ëŠ¥í•˜ë‹¤ê³  ì—¬ê²¨ì¡Œë‹¤.

ğŸ’¡ ê·¸ëŸ¬ë‚˜ ì œí”„ë¦¬ íŒíŠ¼ êµìˆ˜ê°€ ì œí•œëœ ë³¼íŠ¸ë§Œ ë¨¸ì‹ ì´ë¼ëŠ” ì‹ ê²½ë§ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì„ ê°œë°œí•˜ì˜€ê³ , ì´ ë°©ë²•ìœ¼ë¡œ ì‹¬ì¸µ ì‹ ê²½ë§ì„ íš¨ìœ¨ì ìœ¼ë¡œ í•™ìŠµì‹œí‚¬ ìˆ˜ ìˆìŒì„ ì¦ëª…í•˜ë©´ì„œ ë‹¤ì‹œ í•œ ë²ˆ ì‹ ê²½ë§ì´ ì£¼ëª©ë°›ê²Œ ë˜ì—ˆë‹¤.

ê·¸ í›„ ë“œë¡­ì•„ì›ƒ ê¸°ë²•, ReLU ë“±ì˜ í™œì„±í™” í•¨ìˆ˜ë“¤ì´ ê°œë°œë˜ë©´ì„œ ë”¥ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì€ ê¸‰ì†í•œ ë°œì „ì„ ì´ë£¨ì—ˆë‹¤.

ì´ë ‡ê²Œ ë°œì „í•´ì˜¨ ì•Œê³ ë¦¬ì¦˜ì˜ ì¤‘ì‹¬ì—ëŠ” **ì—­ì „íŒŒ(backpropagation)** ê°€ ìˆë‹¤. 

**ì—­ì „íŒŒ**ëŠ” ê°„ë‹¨íˆ ë§í•´, ì¶œë ¥ì¸µì´ ë‚´ë†“ì€ ê²°ê³¼ì˜ ì˜¤ì°¨ë¥¼ ì‹ ê²½ë§ì„ ë”°ë¼ ì…ë ¥ì¸µê¹Œì§€ ì—­ìœ¼ë¡œ ì „íŒŒí•˜ë©° ê³„ì‚°í•´ë‚˜ê°€ëŠ” ë°©ì‹ì´ë‹¤.

ì´ ë°©ì‹ì€ ì…ë ¥ì¸µë¶€í„° ê°€ì¤‘ì¹˜ë¥¼ ì¡°ì ˆí•´ê°€ëŠ” ê¸°ì¡´ ë°©ì‹ë³´ë‹¤ í›¨ì”¬ ìœ ì˜ë¯¸í•œ ë°©ì‹ìœ¼ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ì¡°ì ˆí•´ì¤˜ì„œ ìµœì í™” ê³¼ì •ì´ í›¨ì”¬ ë¹ ë¥´ê³  ì •í™•í•´ì§„ë‹¤.

ì—­ì „íŒŒëŠ” ì‹ ê²½ë§ì„ êµ¬í˜„í•˜ë ¤ë©´ ê±°ì˜ í•­ìƒ ì ìš©í•´ì•¼í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì´ì§€ë§Œ êµ¬í˜„í•˜ê¸°ëŠ” ì¡°ê¸ˆ ì–´ë µë‹¤.

í…ì„œí”Œë¡œëŠ” í™œì„±í™” í•¨ìˆ˜ì™€ í•™ìŠµ í•¨ìˆ˜ ëŒ€ë¶€ë¶„ì— ì—­ì „íŒŒ ê¸°ë²•ì„ ê¸°ë³¸ìœ¼ë¡œ ì œê³µí•œë‹¤. 

í…ì„œí”Œë¡œë¥¼ ì‚¬ìš©í•˜ë©´ ë‹¤ì–‘í•œ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì„ ì§ì ‘ êµ¬í˜„í•˜ì§€ ì•Šê³ ë„ ë§¤ìš° ì‰½ê²Œ ì‹ ê²½ë§ì„ ë§Œë“¤ê³  í•™ìŠµí•  ìˆ˜ ìˆë‹¤.

## 4.2 ê°„ë‹¨í•œ ë¶„ë¥˜ ëª¨ë¸ êµ¬í˜„í•˜ê¸°

ë”¥ëŸ¬ë‹ì€ ë§¤ìš° ë‹¤ì–‘í•œ ë¶„ì•¼ì— ì‚¬ìš©ë˜ì§€ë§Œ ê·¸ ì¤‘ ê°€ì¥ í­ë„“ê²Œ í™œìš©ë˜ëŠ” ë¶„ì•¼ëŠ” íŒ¨í„´ ì¸ì‹ì„ í†µí•œ ì˜ìƒ ì²˜ë¦¬ì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì‚¬ì§„ì´ ê³ ì–‘ì´ì¸ì§€, ê°•ì•„ì§€ì¸ì§€ ë˜ëŠ” ìë™ì°¨ì¸ì§€, ë¹„í–‰ê¸°ì¸ì§€ ë“±ì„ íŒë‹¨í•˜ëŠ” ì¼ì— ì“°ì¸ë‹¤.

ì´ì²˜ëŸ¼ íŒ¨í„´ì„ íŒŒì•…í•´ ì—¬ëŸ¬ ì¢…ë¥˜ë¡œ êµ¬ë¶„í•˜ëŠ” ì‘ì—…ì„ **ë¶„ë¥˜**ë¼ê³  í•œë‹¤.

ì´ë²ˆ ì˜ˆì—ì„œëŠ” í„¸ê³¼ ë‚ ê°œê°€ ìˆëŠëƒë¥¼ ê¸°ì¤€ìœ¼ë¡œ í¬ìœ ë¥˜ì™€ ì¡°ë¥˜ë¥¼ êµ¬ë¶„í•˜ëŠ” ì‹ ê²½ë§ ëª¨ë¸ì„ ë§Œë“¤ì–´ë³¸ë‹¤. 

ì´ë¯¸ì§€ ëŒ€ì‹  ê°„ë‹¨í•œ ì´ì§„ ë°ì´í„°ë¥¼ ì´ìš©í•œë‹¤.

1ï¸âƒ£ í…ì„œí”Œë¡œì™€ Numpy ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„í¬íŠ¸í•œë‹¤. NumpyëŠ” ìˆ˜ì¹˜í•´ì„ìš© íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ì´ë‹¤. í–‰ë ¬ ì¡°ì‘ê³¼ ì—°ì‚°ì— í•„ìˆ˜ë¼ í•  ìˆ˜ ìˆê³  í…ì„œí”Œë¡œë„ Numpyë¥¼ ë§¤ìš° ê¸´ë°€í•˜ê²Œ ì´ìš©í•˜ê³  ìˆë‹¤.

    import tensorflow as tf
    import numpy as np
    
2ï¸âƒ£ ë‹¤ìŒì€ í•™ìŠµì— ì‚¬ìš©í•  ë°ì´í„°ë¥¼ ì •ì˜í•œë‹¤. í„¸ê³¼ ë‚ ê°œê°€ ìˆëŠëƒë¥¼ ë‹´ì€ íŠ¹ì§• ë°ì´í„°ë¥¼ êµ¬ì„±í•œë‹¤. ìˆìœ¼ë©´ 1, ì—†ìœ¼ë©´ 0ì´ë‹¤.

    x_data = np.array([[0,0],[1,0],[0,0][0,0],[0,1]])   # [í„¸, ë‚ ê°œ]
    
  ê·¸ë‹¤ìŒì€ ê° ê°œì²´ê°€ ì‹¤ì œ ì–´ë–¤ ì¢…ë¥˜ì¸ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë ˆì´ë¸” ë°ì´í„°ë¥¼ êµ¬ì„±í•œë‹¤. ì¦‰ ì•ì„œ ì •ì˜í•œ íŠ¹ì§• ë°ì´í„°ì˜ ê° ê°œì²´ê°€ í¬ìœ ë¥˜ì¸ì§€ ì¡°ë¥˜ì¸ì§€, ì•„ë‹ˆë©´ ì œ 3ì˜ ì¢…ë¥˜ì¸ì§€ë¥¼ ê¸°ë¡í•œ ì‹¤ì œ ê²°ê³¼ê°’ì´ë‹¤.
  
  ë ˆì´ë¸” ë°ì´í„°ëŠ” ì›-í•« ì¸ì½”ë”©ì´ë¼ëŠ” íŠ¹ìˆ˜í•œ í˜•íƒœë¡œ êµ¬ì„±í•œë‹¤. ì›-í•« ì¸ì½”ë”©ì´ë€ ë°ì´í„°ê°€ ê°€ì§ˆ ìˆ˜ ìˆëŠ” ê°’ë“¤ì„ ì¼ë ¬ë¡œ ë‚˜ì—´í•œ ë°°ì—´ì„ ë§Œë“¤ê³  ê·¸ ì¤‘ í‘œí˜„í•˜ë ¤ëŠ” ê°’ì„ ëœ»í•˜ëŠ” ì¸ë±ìŠ¤ì˜ ì›ì†Œë§Œ 1ë¡œ í‘œê¸°í•˜ê³  ë‚˜ë¨¸ì§€ ì›ì†ŒëŠ” ëª¨ë‘ 0ìœ¼ë¡œ ì±„ìš°ëŠ” í‘œê¸°ë²•ì´ë‹¤.
  
  ìš°ë¦¬ê°€ íŒë³„í•˜ê³ ì í•˜ëŠ” ê°œì²´ì˜ ì¢…ë¥˜ë¥¼ ì›-í•« ì¸ì½”ë”©ìœ¼ë¡œ ë‚˜íƒ€ë‚´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.
  
    ê¸°íƒ€ = [1,0,0]
    í¬ìœ ë¥˜ = [0,1,0]
    ì¡°ë¥˜ = [0,0,1]

  ì´ë¥¼ íŠ¹ì§• ë°ì´í„°ì™€ ì—°ê´€ ì§€ì–´ ë ˆì´ë¸” ë°ì´í„°ë¡œ êµ¬ì„±í•˜ë©´ ë‹¤ìŒì²˜ëŸ¼ ë§Œë“¤ ìˆ˜ ìˆë‹¤.
  
    y_data = np.array([[1,0,0], [0,1,0], [0,0,1], [1,0,0], [1,0,0], [0,0,1]])
    
3ï¸âƒ£ ì‹ ê²½ë§ ëª¨ë¸ì„ êµ¬ì„±í•´ë³´ì. íŠ¹ì§• Xì™€ ë ˆì´ë¸” Yì™€ì˜ ê´€ê³„ë¥¼ ì•Œì•„ë‚´ëŠ” ëª¨ë¸ì´ë‹¤. ì´ë•Œ Xì™€ Yì— ì‹¤ì¸¡ê°’(ground truth)ë¥¼ ë„£ì–´ì„œ í•™ìŠµì‹œí‚¬ ê²ƒì´ë‹ˆê¹Œ Xì™€ YëŠ” í”Œë ˆì´ìŠ¤í™€ë”ë¡œ ì„¤ì •í•œë‹¤.

    X = tf.placeholder(tf.float32)
    Y = tf.placeholder(tf.float32)
  
  ê·¸ë‹¤ìŒì€ ì•ì„œ ë°°ìš´ ì‹ ê²½ë§ì„ ê²°ì •í•˜ëŠ” ê°€ì¤‘ì¹˜ì™€ í¸í–¥ ë³€ìˆ˜ë¥¼ ì„¤ì •í•œë‹¤. ì´ ë³€ìˆ˜ë“¤ì˜ ê°’ì„ ì—¬ëŸ¬ê°€ì§€ë¡œ ë°”ê¿”ê°€ë©´ì„œ Xì™€ Yì˜ ì—°ê´€ ê´€ê³„ë¥¼ í•™ìŠµí•˜ê²Œ ëœë‹¤.
  
    W = tf.Variable(tf.random_uniform([2,3], -1.,1.))   # [íŠ¹ì§• ìˆ˜(ì…ë ¥ì¸µ), ë ˆì´ë¸” ìˆ˜(ì¶œë ¥ì¸µ)]ì˜ êµ¬ì„±ì¸ [2,3]ìœ¼ë¡œ ì„¤ì •
    b = tf.Variable(tf.zeros([3]))   # ë ˆì´ë¸” ìˆ˜ì¸ 3ê°œì˜ ìš”ì†Œë¥¼ ê°€ì§„ ë³€ìˆ˜ë¡œ ì„¤ì •

  ê°€ì¤‘ì¹˜ë¥¼ ê³±í•˜ê³  í¸í–¥ì„ ë”í•œ ê²°ê³¼ë¥¼ í™œì„±í™” í•¨ìˆ˜ì¸ ReLUì— ì ìš©í•˜ë©´ ì‹ ê²½ë§ êµ¬ì„±ì€ ëì´ë‹¤.
  
    L = tf.add(tf.matmul(X,W),b)
    L = tf.nn.relu(L)

4ï¸âƒ£ ì¶”ê°€ë¡œ ì‹ ê²½ë§ì„ í†µí•´ ë‚˜ì˜¨ ì¶œë ¥ê°’ì„ softmaxí•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ ì‚¬ìš©í•˜ê¸° ì‰½ê²Œ ë‹¤ë“¬ì–´ì¤€ë‹¤.

    model = tf.nn.softmax(L)  # ê°ê°ì€ í•´ë‹¹ ê²°ê³¼ì˜ í™•ë¥ ë¡œ í•´ì„
    
5ï¸âƒ£ ì´ì œ ì†ì‹¤í•¨ìˆ˜ë¥¼ ì‘ì„±í•œë‹¤. ì†ì‹¤ í•¨ìˆ˜ëŠ” ì›-í•« ì¸ì½”ë”©ì„ ì´ìš©í•œ ëŒ€ë¶€ë¶„ì˜ ëª¨ë¸ì—ì„œ ì‚¬ìš©í•˜ëŠ” **êµì°¨ ì—”íŠ¸ë¡œí”¼**í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë„ë¡ í•œë‹¤. êµì°¨ ì—”íŠ¸ë¡œí”¼ ê°’ì€ ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ ì‚¬ì´ì˜ í™•ë¥  ë¶„í¬ ì°¨ì´ë¥¼ ê³„ì‚°í•œ ê°’ìœ¼ë¡œ ê¸°ë³¸ ì½”ë“œëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

    cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(model),axis = 1))   # reduce_xxx í•¨ìˆ˜ë“¤ì€ í…ì„œì˜ ì°¨ì›ì„ ì¤„ì„

 ì½”ë“œê°€ ë³µì¡í•´ ë³´ì´ì§€ë§Œ ê³„ì‚°ëœ ê°’ì„ ë³´ë©´ ì–´ë µì§€ ì•Šê²Œ ì´í•´í•  ìˆ˜ ìˆë‹¤. ê³„ì‚° ê³¼ì •ì„ ì²œì²œíˆ ë”°ë¼ê°€ë³´ì. ë¨¼ì € YëŠ” ì‹¤ì¸¡ê°’ì´ë‹¤. ê·¸ë¦¬ê³  modelì€ ì‹ ê²½ë§ì„ í†µí•´ ë‚˜ì˜¨ ì˜ˆì¸¡ê°’ì´ë‹¤.
 
        Y              model
    [[1 0 0 ]     [[0.1 0.7 0.2]
     [0 1 0]]      [0.2 0.8 0.0]]
 
 ê·¸ëŸ° ë‹¤ìŒ modelê°’ì— logë¥¼ ì·¨í•œ ê°’ì„ Yë‘ ê³±í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì´ ëœë‹¤.

        Y              model           Y * tf.log(model)
    [[1 0 0 ]     [[0.1 0.7 0.2]   ->   [[-1.0 0  0]
     [0 1 0]]      [0.2 0.8 0.0]]       [0 -0.09 0]]
     
 ì´ì œ í–‰ë³„ë¡œ ê°’ì„ ë‹¤ ë”í•œë‹¤.

    Y * tf.log(model)        reduce_sum(axis = 1)
     [[-1.0 0  0]       ->     [ -1.0  -0.09]
     [0 -0.09 0]]

 ë§ˆì§€ë§‰ìœ¼ë¡œ ë°°ì—´ ì•ˆ ê°’ì˜ í‰ê· ì„ ë‚´ë©´ ê·¸ê²ƒì´ ë°”ë¡œ ìš°ë¦¬ì˜ ì†ì‹¤ê°’ì¸ êµì°¨ ì—”íŠ¸ë¡œí”¼ ê°’ì´ ëœë‹¤.
 
    reduce_sum(axis = 1)      reduce_mean
      [ -1.0  -0.09]      ->     -0.545
   
6ï¸âƒ£ ì´ì œ í•™ìŠµì„ ì‹œì¼œë³´ì. í•™ìŠµì€ í…ì„œí”Œë¡œê°€ ê¸°ë³¸ ì œê³µí•˜ëŠ” ìµœì í™” í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë©´ ëœë‹¤.

    # ê²½ì‚¬í•˜ê°•ë²•ìœ¼ë¡œ ìµœì í™”
    optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.01)
    train_op = optimizer.minimize(cost)
    
    # í…ì„œí”Œë¡œì˜ ì„¸ì…˜ì„ ì´ˆê¸°í™”
    init = tf.global_variables_initializer()
    sess = tf.Session()
    sess.run(init)
    
    # íŠ¹ì§•ì™€ ë ˆì´ë¸” ë°ì´í„°ë¥¼ ì´ìš©í•´ 100ë²ˆ í•™ìŠµ
    for step in range(100):
      sess.run(train_op, feed_dict({X:x_data, Y: y_data}))
      
      # í•™ìŠµ ë„ì¤‘ 10ë²ˆì— í•œ ë²ˆ ì”© ì†ì‹¤ê°’ ì¶œë ¥
      if (step+1) % 10 == 0:
        print(step +1, sess.run(cost,feed_dict = {X:x_data, Y: y_data}))

7ï¸âƒ£ í•™ìŠµëœ ê²°ê³¼ë¥¼ í™•ì¸í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•œë‹¤.

    prediction = tf.argmax(model, axis = 1)
    target = tf.argmax(Y, axis = 1)
    print('ì˜ˆì¸¡ê°’:', sess.run(prediction, feed_dict = {X:x_data}))
    print('ì‹¤ì œê°’:', sess.run(target, feed_dict = {Y:y_data}))
    
  - ì˜ˆì¸¡ê°’ì¸ modelì„ ë°”ë¡œ ì¶œë ¥í•˜ë©´ [0.2 0.7 0.1]ê³¼ ê°™ì´ í™•ë¥ ë¡œ ë‚˜ì˜¤ê¸° ë•Œë¬¸ì— ìš”ì†Œ ì¤‘ ê°€ì¥ í° ì¸ë±ìŠ¤ë¥¼ ì°¾ì•„ì£¼ëŠ” argmaxí•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë ˆì´ë¸” ê°’ì„ ì¶œë ¥í•˜ë„ë¡ í–ˆë‹¤. ì¦‰ ë‹¤ìŒì²˜ëŸ¼ ì›-í•« ì¸ì½”ë”©ì„ ê±°ê¾¸ë¡œ í•œ ê²°ê³¼ë¥¼ ë§Œë“¤ì–´ì¤€ë‹¤.

        [[0 1 0] [1 0 0]]  -> [1 0]
        [[0.2 0.7 0.1] [0.9 0.1 0.]] -> [1 0]

8ï¸âƒ£ ì •í™•ë„ë¥¼ ì¶œë ¥í•´ë³¸ë‹¤.

    is_correct = tf.equal(prediction, target)
    accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))  # true / falseë¡œ ë‚˜ì˜¨ ê²°ê³¼ë¥¼ ë‹¤ì‹œ tf.castí•¨ìˆ˜ë¥¼ ì´ìš©í•´ 0ê³¼ 1ë¡œ ë°”ê¾¸ì–´ í‰ê· ì„ ë‚´ë©´ ê°„ë‹¨íˆ ì •í™•ë„ êµ¬í•  ìˆ˜ ìˆìŒ
    print('ì •í™•ë„: %.2f' % sess.run(accuracy * 100, feed_dict={X:x_data,Y:y_data}))

ğŸ“ ì „ì²´ ì½”ë“œ ğŸ“

    import tensorflow as tf
    import numpy as np
    
    x_data = np.array([[0,0], [1,0], [1,1], [0,0], [0,0], [0,1]])
    y_data = np.array([[1,0,0], [0,1,0], [0,0,1], [1,0,0], [1,0,0], [0,0,1]])
    
    X = tf.placeholder(tf.float32)
    Y = tf.placeholder(tf.float32)
    
    W = tf.Variable(tf.random_uniform([2,3], -1. , 1.))
    b = tf.Variable(tf.zeros([3]))
    
    L = tf.add(tf.matmul(X,W),b)
    L = tf.nn.relu(L)
    
    model = tf.nn.softmax(L)
    cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(model), axis =1))
    
    optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.01)
    train_op = optimizer.minimize(cost)
    
    init = tf.global_variables_initializer()
    sess = tf.Session()
    sess.run(init)
    
    for step in range(100):
      sess.run(train_op, feed_dict = {X:x_data, Y:y_data})
      
      if (step+1) % 10 == 0:
        print(step +1, sess.run(cost, feed_dict = {X:x_data, Y:y_data}))
        
    prediction = tf.argmax(model, axis =1)
    target = tf.argmax(Y, axis =1)
    print('ì˜ˆì¸¡ê°’: ', sess.run(prediction, feed_dict = {X:x_data}))
    print('ì‹¤ì œê°’: ', sess.run(target, feed_dict = {Y:y_data}))
    
    is_correct = tf.equal(prediction, target)
    accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))
    print('ì •í™•ë„: %.2f' % sess.run(accuracy *100, feed_dict={X:x_data, Y:y_data}))
    
## 4.3 ì‹¬ì¸µ ì‹ ê²½ë§ êµ¬í˜„í•˜ê¸°

ì´ì œ ì‹ ê²½ë§ì˜ ì¸µì„ ë‘˜ ì´ìƒìœ¼ë¡œ êµ¬ì„±í•œ ì‹¬ì¸µ ì‹ ê²½ë§, ì¦‰ ë”¥ëŸ¬ë‹ì„ êµ¬í˜„í•´ë³¸ë‹¤.

1ï¸âƒ£ ë‹¤ì¸µ ì‹ ê²½ë§ì„ ë§Œë“œëŠ” ê²ƒì€ ë§¤ìš° ê°„ë‹¨í•˜ë‹¤. ì•ì„œ ë§Œë“  ì‹ ê²½ë§ ëª¨ë¸ì— ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ ì¶”ê°€í•˜ê¸°ë§Œ í•˜ë©´ ëœë‹¤.

    W1 = tf.Variable(tf.random_uniform([2,10], -1. , 1.))
    W2 = tf.Variable(tf.random_uniform([10,3], -1. , 1.))
    
    b1 = tf.Variable(tf.zeros([10]))
    b2 = tf.Variable(tf.zeros([3]))
    
  ì½”ë“œë¥¼ ë³´ë©´ ì²« ë²ˆì§¸ ê°€ì¤‘ì¹˜ í˜•íƒœëŠ” [2,10]ìœ¼ë¡œ, ë‘ ë²ˆì§¸ ê°€ì¤‘ì¹˜ëŠ” [10,3]ìœ¼ë¡œ ì„¤ì •í–ˆê³  í¸í–¥ì„ ê°ê° 10ê³¼ 3ìœ¼ë¡œ ì„¤ì •í–ˆë‹¤. ê·¸ ì˜ë¯¸ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.
  
    # ê°€ì¤‘ì¹˜
    W1 = [2,10]  -> [íŠ¹ì§•, ì€ë‹‰ì¸µì˜ ë‰´ëŸ° ìˆ˜]
    W2 = [10,3]  -> [ì€ë‹‰ ì¸µì´ ë‰´ëŸ° ìˆ˜, ë¶„ë¥˜ ìˆ˜]
    
    # í¸í–¥
    b1 = [10]   -> ì€ë‹‰ì¸µì˜ ë‰´ëŸ° ìˆ˜
    b2 = [3]   -> ë¶„ë¥˜ ìˆ˜
    
  ì…ë ¥ì¸µê³¼ ì¶œë ¥ì¸µì€ ê°ê° íŠ¹ì§•ê³¼ ë¶„ë¥˜ ê°œìˆ˜ë¡œ ë§ì¶”ê³  ì¤‘ê°„ì˜ ì—°ê²° ë¶€ë¶€ì€ ë§ë‹¿ì€ ì¸µì˜ ë‰´ëŸ° ìˆ˜ì™€ ê°™ë„ë¡ ë§ì¶”ë©´ ëœë‹¤. ì¤‘ê°„ì˜ ì—°ê²° ë¶€ë¶„ì„ **ì€ë‹‰ì¸µ**ì´ë¼ê³  í•˜ë©° ì€ë‹‰ì¸µì˜ ë‰´ëŸ° ìˆ˜ëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„°ì´ë‹ˆ ì‹¤í—˜ì„ í†µí•´ ê°€ì¥ ì ì ˆí•œ ìˆ˜ë¥¼ ì •í•˜ë©´ ëœë‹¤.
  
2ï¸âƒ£ íŠ¹ì§• ì…ë ¥ê°’ì— ì²« ë²ˆì§¸ ê°€ì¤‘ì¹˜ì™€ í¸í–¥, ê·¸ë¦¬ê³  í™œì„±í™” í•¨ìˆ˜ë¥¼ ì ìš©í•œë‹¤.

    L1 = tf.add(tf.matmul(X,W1), b1)
    L1 = tf.nn.relu(L1)
    
3ï¸âƒ£ ì¶œë ¥ì¸µì„ ë§Œë“¤ê¸° ìœ„í•´ ë‘ ë²ˆì§¸ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ ì ìš©í•˜ì—¬ ìµœì¢… ëª¨ë¸ì„ ë§Œë“ ë‹¤. ì€ë‹‰ì¸µì— ë‘ ë²ˆì§¸ ê°€ì¤‘ì¹˜ W2ì™€ í¸í–¥ b2ë¥¼ ì ìš©í•˜ë©´ ìµœì¢…ì ìœ¼ë¡œ 3ê°œì˜ ì¶œë ¥ê°’ì„ ê°€ì§€ê²Œ ëœë‹¤.

    model = tf.add(tf.matmul(L1,W2), b2)
    
4ï¸âƒ£ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì‘ì„±í•œë‹¤. ì†ì‹¤ í•¨ìˆ˜ëŠ” êµì°¨ ì—”íŠ¸ë¡œí”¼ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œë‹¤. ë‹¤ë§Œ ì´ë²ˆì—ëŠ” í…ì„œí”Œë¡œê°€ ê¸°ë³¸ ì œê³µí•˜ëŠ” êµì°¨ ì—”íŠ¸ë¡œí”¼ í•¨ìˆ˜ë¥¼ ì´ìš©í•œë‹¤, 

    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels = Y, logits = model))
    
    optimizer = tf.train.AdamOptimizer(learning_rate = 0.01)   # ê²½ì‚¬í•˜ê°•ë²•ë³´ë‹¤ ë³´í¸ì ìœ¼ë¡œ ì„±ëŠ¥ì´ ì¢‹ìŒ
    train_op = optimizer.minimize(cost)
  
5ï¸âƒ£ í•™ìŠµ ì§„í–‰, ì†ì‹¤ê°’ê³¼ ì •í™•ë„ ì¸¡ì • ë“± ì•ì ˆì—ì„œ ë³¸ ë‚˜ë¨¸ì§€ ì½”ë“œë¥¼ ë„£ê³  ì‹¤í–‰í•˜ë©´ ì •í™•í•œ ì˜ˆì¸¡ê°’ì„ ì–»ê²Œ ë  ê²ƒì´ë‹¤.

ğŸ“ ì „ì²´ ì½”ë“œ ğŸ“

    import tensorflow as tf
    import numpy as np
    
    x_data = np.array([[0,0], [1,0], [1,1], [0,0], [0,0], [0,1]])
    y_data = np.array([1,0,0], [0,1,0], [0,0,1], [1,0,0], [1,0,0], [0,0,1]])
  
    X = tf.placeholder(tf.float32)
    Y = tf.placeholder(tf.float32)  
  
    W1 = tf.Variable(tf.random_uniform([2,10], -1. , 1.))
    W2 = tf.Variable(tf.random_uniform([10,3], -1. , 1.))
    
    b1 = tf.Variable(tf.zeros([10]))
    b2 = tf.Variable(tf.zeros([3])) 
  
    L1 = tf.add(tf.matmul(X,W1), b1)
    L1 = tf.nn.relu(L1)  
    
    model = tf.add(tf.matmul(L1,W2), b2)
    
    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels = Y, logits = model))
    
    optimizer = tf.train.AdamOptimizer(learning_rate = 0.01)   # ê²½ì‚¬í•˜ê°•ë²•ë³´ë‹¤ ë³´í¸ì ìœ¼ë¡œ ì„±ëŠ¥ì´ ì¢‹ìŒ
    train_op = optimizer.minimize(cost)   
    
    init = tf.global_variables_initializer()
    sess = tf.Session()
    sess.run(init)
    
    for step in range(100):
      sess.run(train_op, feed_dict = {X:x_data, Y:y_data})
      
      if (step+1) % 10 == 0::
        print(step +1, sess.run(cost, feed_dict = {X:x_data, Y:y_data}))
        
    prediction = tf.argmax(model, axis =1)
    target = tf.argmax(Y, axis =1)
    print('ì˜ˆì¸¡ê°’: ', sess.run(prediction, feed_dict = {X:x_data}))
    print('ì‹¤ì œê°’: ', sess.run(target, feed_dict = {Y:y_data}))
    
    is_correct = tf.equal(prediction, target)
    accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))
    print('ì •í™•ë„: %.2f' % sess.run(accuracy *100, feed_dict={X:x_data, Y:y_data}))    
